{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e33ce0dd-d50f-4ff4-b441-8c0ca112da2a",
   "metadata": {},
   "source": [
    "# Email Search AI"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85dfb6d5-861d-4823-bbcc-cf7283266703",
   "metadata": {},
   "source": [
    "<img src=\"email_search_ai.png\" style=\"display:block; margin-left:auto; margin-right:auto;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa3ac6d9-d4dc-4768-9908-98ebf8237e79",
   "metadata": {},
   "source": [
    "## Problem Statement"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7393f3d8-c376-47fc-9b5e-26d5aae0b123",
   "metadata": {},
   "source": [
    "In enterprise environments, email threads often contain critical discussions, decisions, and context across multiple stakeholders. However, locating specific information within large, unstructured, and nested email threads is time-consuming and inefficient using conventional keyword-based search tools. Professionals face challenges in extracting relevant insights without wading through entire conversations manually. There is a pressing need for an intelligent system that enables **semantic search** and **automated summarization** of email threads."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e6b6599-9758-44a5-903b-2c17c91771e1",
   "metadata": {},
   "source": [
    "## Project Overview"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d852373-9aa4-497f-bf0a-65fef70f3d74",
   "metadata": {},
   "source": [
    "**HelpMate_Email Search_AI** is an end-to-end **RAG-based AI assistant** designed for semantic search and summarization of email threads. It uses **Sentence Transformers** for creating dense **vector embeddings** of email content, stores them in a **ChromaDB vector store**, and enables **semantic index search**. Upon receiving a user query, it retrieves the most relevant chunks using **vector similarity**, improves result quality through **cross-encoder-based reranking**, and finally, generates context-aware responses using **OpenAI's LLM (e.g., GPT-3.5-turbo)**.\n",
    "\n",
    "The system employs a **Retrieval-Augmented Generation (RAG)** architecture with **caching** for efficiency and performance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a00a5e8-8114-41e2-b546-826bcd18ef38",
   "metadata": {},
   "source": [
    "## Project Objectives"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72b5d0f1-0f93-4758-81d4-a9ab82a9f3c5",
   "metadata": {},
   "source": [
    "- **Semantic Understanding of Emails**: Use **Sentence Transformers (all-MiniLM-L6-v2)** to convert email chunks into vector representations capturing semantic meaning.\n",
    "\n",
    "- **Vector Database Indexing**: Store email embeddings in **ChromaDB**, enabling fast approximate nearest neighbor (ANN) vector search.\n",
    "\n",
    "- **Semantic Search & Retrieval**: Support user queries via **embedding-based similarity search** across indexed email chunks.\n",
    "\n",
    "- **Result Reranking**: Improve retrieval accuracy with **cross-encoder reranking (ms-marco-MiniLM-L-6-v2)**, scoring relevance between query and result pairs.\n",
    "\n",
    "- **Contextual Answer Generation**: Use a **Retrieval-Augmented Generation (RAG)** pipeline to feed retrieved results into OpenAI GPT models for answer synthesis.\n",
    "\n",
    "- **Query Caching**: Implement a file-based **caching** layer to store query results and avoid repeated computations."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "718433c1-8f5a-4922-8b14-b610a63373da",
   "metadata": {},
   "source": [
    "## Functional Features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e446c9c1-cd6f-4b10-b0aa-866cc72ea437",
   "metadata": {},
   "source": [
    "\n",
    "| **Component**                   | **Description**                                                                 | **Technology Used**                             |\n",
    "|--------------------------------|---------------------------------------------------------------------------------|-------------------------------------------------|\n",
    "| **Email Preprocessing**        | Cleans raw email bodies by removing quoted replies and normalizing text         | `Regex`, custom cleaning                        |\n",
    "| **Chunking**                   | Splits cleaned emails into overlapping token-limited chunks                     | Custom logic                                    |\n",
    "| **Embeddings**                 | Transforms email chunks into dense semantic vectors                             | `SentenceTransformer` (`all-MiniLM-L6-v2`)      |\n",
    "| **Vector Indexing**            | Stores and indexes embeddings for fast similarity search                        | `ChromaDB`                                      |\n",
    "| **Query Embedding**            | Embeds natural language queries for semantic comparison                         | `SentenceTransformer`                           |\n",
    "| **Initial Vector Search**      | Retrieves top-N similar email chunks using ANN search                           | `ChromaDB`                                      |\n",
    "| **Reranking**                  | Reorders retrieved results by true semantic relevance                           | `CrossEncoder` (`ms-marco-MiniLM-L-6-v2`)       |\n",
    "| **Retrieval-Augmented Generation (RAG)** | Combines retrieved chunks with the query to form a prompt for GPT      | `OpenAI GPT-3.5-turbo`                          |\n",
    "| **Answer Generation**          | Synthesizes a coherent answer based on context                                  | `OpenAI Chat Completion API`                    |\n",
    "| **Caching**                    | Stores query results using hashed query keys for faster repeat access           | JSON file-based custom `Cache` class            |\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bfd8630-3e52-4149-9a29-ee1bf4b6e6ff",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "db4fb8c1-e9cf-4c4f-b3be-15326f8101e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Admin\\anaconda3\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "# Import the libraries\n",
    "import os\n",
    "import re\n",
    "import pandas as pd\n",
    "from typing import List, Dict\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import chromadb\n",
    "from chromadb.config import Settings\n",
    "from chromadb.utils import embedding_functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7f5017b7-6c7e-4712-8c6a-49172d053321",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Install required modules\n",
    "!pip3 install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3bd25ece-0ec2-45ff-93cc-4522847a9793",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.95.1\n"
     ]
    }
   ],
   "source": [
    "#Let's check the version of OpenAI\n",
    "import openai\n",
    "print(openai.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "79dacc8b-d521-41a1-9d26-92777ee18fbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read key from text file\n",
    "with open(\"openai_key.txt\", \"r\") as f:\n",
    "    api_key = f.read().strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "12cf2b9a-5fa0-48dc-8ab0-5d1b9be4a007",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pass key to OpenAI client\n",
    "from openai import OpenAI\n",
    "client = OpenAI(api_key=api_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "80bfa77e-effa-4aec-a400-5583335af460",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>thread_id</th>\n",
       "      <th>subject</th>\n",
       "      <th>timestamp</th>\n",
       "      <th>from</th>\n",
       "      <th>to</th>\n",
       "      <th>body</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>FW: Master Termination Log</td>\n",
       "      <td>2002-01-29 11:23:42</td>\n",
       "      <td>Gossett, Jeffrey C. JGOSSET</td>\n",
       "      <td>['Giron', 'Darron C. Dgiron', 'Love', 'Phillip...</td>\n",
       "      <td>\\n\\n -----Original Message-----\\nFrom: =09Ther...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>FW: Master Termination Log</td>\n",
       "      <td>2002-01-31 12:50:00</td>\n",
       "      <td>Theriot, Kim S. KTHERIO</td>\n",
       "      <td>['Murphy', 'Melissa Mmurphy', 'Gossett', 'Jeff...</td>\n",
       "      <td>\\n\\n -----Original Message-----\\nFrom: =09Panu...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>FW: Master Termination Log</td>\n",
       "      <td>2002-02-05 15:03:35</td>\n",
       "      <td>Theriot, Kim S. KTHERIO</td>\n",
       "      <td>['Murphy', 'Melissa Mmurphy', 'Anderson', 'Dia...</td>\n",
       "      <td>Note to Stephanie Panus....\\n\\nStephanie...ple...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>FW: Master Termination Log</td>\n",
       "      <td>2002-02-05 15:06:25</td>\n",
       "      <td>Theriot, Kim S. KTHERIO</td>\n",
       "      <td>['Hall', 'D. Todd Thall', 'Sweeney', 'Kevin Ks...</td>\n",
       "      <td>\\n\\n -----Original Message-----\\nFrom: =09Panu...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>FW: Master Termination Log</td>\n",
       "      <td>2002-05-28 07:20:35</td>\n",
       "      <td>Kelly, Katherine L. KKELLY</td>\n",
       "      <td>['Germany', 'Chris Cgerman']</td>\n",
       "      <td>\\n\\n -----Original Message-----\\nFrom: =09McMi...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   thread_id                     subject            timestamp  \\\n",
       "0          1  FW: Master Termination Log  2002-01-29 11:23:42   \n",
       "1          1  FW: Master Termination Log  2002-01-31 12:50:00   \n",
       "2          1  FW: Master Termination Log  2002-02-05 15:03:35   \n",
       "3          1  FW: Master Termination Log  2002-02-05 15:06:25   \n",
       "4          1  FW: Master Termination Log  2002-05-28 07:20:35   \n",
       "\n",
       "                          from  \\\n",
       "0  Gossett, Jeffrey C. JGOSSET   \n",
       "1      Theriot, Kim S. KTHERIO   \n",
       "2      Theriot, Kim S. KTHERIO   \n",
       "3      Theriot, Kim S. KTHERIO   \n",
       "4   Kelly, Katherine L. KKELLY   \n",
       "\n",
       "                                                  to  \\\n",
       "0  ['Giron', 'Darron C. Dgiron', 'Love', 'Phillip...   \n",
       "1  ['Murphy', 'Melissa Mmurphy', 'Gossett', 'Jeff...   \n",
       "2  ['Murphy', 'Melissa Mmurphy', 'Anderson', 'Dia...   \n",
       "3  ['Hall', 'D. Todd Thall', 'Sweeney', 'Kevin Ks...   \n",
       "4                       ['Germany', 'Chris Cgerman']   \n",
       "\n",
       "                                                body  \n",
       "0  \\n\\n -----Original Message-----\\nFrom: =09Ther...  \n",
       "1  \\n\\n -----Original Message-----\\nFrom: =09Panu...  \n",
       "2  Note to Stephanie Panus....\\n\\nStephanie...ple...  \n",
       "3  \\n\\n -----Original Message-----\\nFrom: =09Panu...  \n",
       "4  \\n\\n -----Original Message-----\\nFrom: =09McMi...  "
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Read the input dataset\n",
    "df_email_thread = pd.read_csv(\"email_dataset/email_thread_details.csv\")\n",
    "df_email_thread.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e3f38fd1-49e4-48e9-a432-4c882b3b30a6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 21684 entries, 0 to 21683\n",
      "Data columns (total 6 columns):\n",
      " #   Column     Non-Null Count  Dtype \n",
      "---  ------     --------------  ----- \n",
      " 0   thread_id  21684 non-null  int64 \n",
      " 1   subject    21684 non-null  object\n",
      " 2   timestamp  21684 non-null  object\n",
      " 3   from       21684 non-null  object\n",
      " 4   to         21684 non-null  object\n",
      " 5   body       21684 non-null  object\n",
      "dtypes: int64(1), object(5)\n",
      "memory usage: 1016.6+ KB\n"
     ]
    }
   ],
   "source": [
    "df_email_thread.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bb7a67a-6a86-402a-9762-a4588c735600",
   "metadata": {},
   "source": [
    "**Description**:\n",
    "The email_thread_details file provides a detailed perspective on individual email threads, encompassing crucial information such as subject, timestamp, sender, recipients, and the content of the email.\n",
    "\n",
    "**Columns**:\n",
    "- **thread_id**: A unique identifier for each email thread.\n",
    "- **subject**: Subject of the email thread.\n",
    "- **timestamp**: Timestamp indicating when the message was sent.\n",
    "- **from**: Sender of the email.\n",
    "- **to**: List of recipients of the email.\n",
    "- **body**: Content of the email message."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ac00b3c-6dfb-4f9a-a2a4-d6d0c128abd1",
   "metadata": {},
   "source": [
    "## Overall Structure  of the code"
   ]
  },
  {
   "cell_type": "raw",
   "id": "04f964b4-ebeb-4dc0-9ad9-082c38ccc501",
   "metadata": {},
   "source": [
    "email-search-ai/\n",
    "│\n",
    "├── email_dataset/\n",
    "│   └── email_thread_summary.csv \n",
    "│\n",
    "├── src/\n",
    "│   ├── embedding_layer.py\n",
    "│   ├── search_layer.py\n",
    "│   ├── cache.py\n",
    "│   ├── generation_layer.py\n",
    "│   └── utils.py\n",
    "│\n",
    "├── outputs/\n",
    "│   ├── search_screenshots/\n",
    "│   ├── generation_screenshots/\n",
    "│\n",
    "├── docs/\n",
    "│   └── project_documentation.md\n",
    "│\n",
    "├── main.py\n",
    "└── requirements.txt\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "123c8270-d708-427c-8907-31683ae8e041",
   "metadata": {},
   "source": [
    "## Embedding Layer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c4b9ef0-ed2a-4c33-a25c-8812539f66fc",
   "metadata": {},
   "source": [
    "<img src=\"embedding_layer_flow.jpg\" style=\"display:block; margin-left:auto; margin-right:auto;\"/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "8c77d2e3-b28f-46a1-a4bd-9852cef8a2df",
   "metadata": {},
   "outputs": [],
   "source": [
    "# src/embedding_layer.py\n",
    "\n",
    "# Import the libraries\n",
    "import os\n",
    "import re\n",
    "import pandas as pd\n",
    "from typing import List, Dict\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import chromadb\n",
    "from chromadb.config import Settings\n",
    "from chromadb.utils import embedding_functions\n",
    "\n",
    "# === CLEANING FUNCTIONS ===\n",
    "\n",
    "def clean_email_body(body: str) -> str:\n",
    "    body = re.sub(r'[\\r\\n]+', ' ', body)  # remove newlines\n",
    "    body = re.sub(r'\\s+', ' ', body)  # normalize spaces\n",
    "    body = re.sub(r'On .* wrote:', '', body)  # remove quoted replies\n",
    "    return body.strip()\n",
    "\n",
    "\n",
    "# === CHUNKING STRATEGY ===\n",
    "\n",
    "def chunk_text(text: str, max_tokens: int = 512, overlap: int = 50) -> List[str]:\n",
    "    words = text.split()\n",
    "    chunks = []\n",
    "    for i in range(0, len(words), max_tokens - overlap):\n",
    "        chunk = ' '.join(words[i:i + max_tokens])\n",
    "        if len(chunk.split()) > 10:\n",
    "            chunks.append(chunk)\n",
    "    return chunks\n",
    "\n",
    "\n",
    "# === EMBEDDING + CHROMA ===\n",
    "\n",
    "class EmbeddingProcessor:\n",
    "    def __init__(self, client, chroma_path: str = \"chroma_db\", model_name: str = \"all-MiniLM-L6-v2\"):\n",
    "        self.model = SentenceTransformer(model_name)\n",
    "        self.client = client\n",
    "        self.chroma_collection = self.client.get_or_create_collection(name=\"email_chunks\")\n",
    "\n",
    "    def process_emails(self, df: pd.DataFrame):\n",
    "        all_chunks = []\n",
    "        metadatas = []\n",
    "\n",
    "        for idx, row in df.iterrows():\n",
    "            cleaned_body = clean_email_body(row['body'])\n",
    "            chunks = chunk_text(cleaned_body)\n",
    "\n",
    "            for i, chunk in enumerate(chunks):\n",
    "                chunk_id = f\"{row['thread_id']}_{idx}_{i}\"\n",
    "                all_chunks.append({\n",
    "                    \"id\": chunk_id,\n",
    "                    \"text\": chunk,\n",
    "                    \"metadata\": {\n",
    "                        \"thread_id\": row[\"thread_id\"],\n",
    "                        \"subject\": row[\"subject\"],\n",
    "                        \"from\": row[\"from\"],\n",
    "                        \"timestamp\": row[\"timestamp\"]\n",
    "                    }\n",
    "                })\n",
    "\n",
    "        print(f\"Embedding {len(all_chunks)} chunks...\")\n",
    "        embeddings = self.model.encode([c['text'] for c in all_chunks], show_progress_bar=True).tolist()\n",
    "\n",
    "        self.chroma_collection.add(\n",
    "            documents=[c['text'] for c in all_chunks],\n",
    "            embeddings=embeddings,\n",
    "            metadatas=[c['metadata'] for c in all_chunks],\n",
    "            ids=[c['id'] for c in all_chunks]\n",
    "        )\n",
    "        \n",
    "\n",
    "    def get_collection(self):\n",
    "        return self.chroma_collection\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "229a5988-4aef-4ea3-8f88-20cbdce958aa",
   "metadata": {},
   "source": [
    "## Cache Layer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fe58e82-38a8-4d5a-a5e8-ccf71708906c",
   "metadata": {},
   "source": [
    "<img src=\"cache_layer_flow1.jpg\" style=\"display:block; margin-left:auto; margin-right:auto;\"/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "2a45265f-cc17-46db-9adc-33d9ea5b29f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# src/cache.py\n",
    "\n",
    "import json\n",
    "import os\n",
    "\n",
    "\n",
    "class Cache:\n",
    "    def __init__(self, cache_file: str):\n",
    "        self.cache_file = cache_file\n",
    "        if os.path.exists(cache_file):\n",
    "            with open(cache_file, \"r\") as f:\n",
    "                self.cache = json.load(f)\n",
    "        else:\n",
    "            self.cache = {}\n",
    "\n",
    "    def contains(self, key: str) -> bool:\n",
    "        return key in self.cache\n",
    "\n",
    "    def get(self, key: str):\n",
    "        return self.cache.get(key, None)\n",
    "\n",
    "    def set(self, key: str, value):\n",
    "        self.cache[key] = value\n",
    "        self._save()\n",
    "\n",
    "    def _save(self):\n",
    "        with open(self.cache_file, \"w\") as f:\n",
    "            json.dump(self.cache, f, indent=2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe11c0f6-29b2-43e6-b9a1-efe855d488be",
   "metadata": {},
   "source": [
    "## Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "d806d803-07f3-47db-b0f6-46e9bbcd79f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def convert_np_types(obj):\n",
    "    if isinstance(obj, dict):\n",
    "        return {k: convert_np_types(v) for k, v in obj.items()}\n",
    "    elif isinstance(obj, list):\n",
    "        return [convert_np_types(i) for i in obj]\n",
    "    elif isinstance(obj, np.float32) or isinstance(obj, np.float64):\n",
    "        return float(obj)\n",
    "    elif isinstance(obj, np.int32) or isinstance(obj, np.int64):\n",
    "        return int(obj)\n",
    "    else:\n",
    "        return obj\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19d215b6-d29d-4746-b49f-7f57fd0951f4",
   "metadata": {},
   "source": [
    "## Search Layer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "834fbdf3-bf5d-49e5-863e-0212014a6c4f",
   "metadata": {},
   "source": [
    "<img src=\"search_layer_flow.jpg\" style=\"display:block; margin-left:auto; margin-right:auto;\"/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "2cd3b0c1-c451-4be0-b8dc-4830eb944f09",
   "metadata": {},
   "outputs": [],
   "source": [
    "# src/search_layer.py\n",
    "\n",
    "import hashlib\n",
    "import json\n",
    "import os\n",
    "from typing import List, Dict\n",
    "\n",
    "import numpy as np\n",
    "from sentence_transformers import SentenceTransformer, CrossEncoder\n",
    "import chromadb\n",
    "from chromadb.config import Settings\n",
    "\n",
    "#from .cache import Cache\n",
    "\n",
    "# === CONFIGURATION ===\n",
    "CACHE_PATH = \"cache/search_cache.json\"\n",
    "CHROMA_PATH = \"chroma_db\"\n",
    "\n",
    "class SearchEngine:\n",
    "    def __init__(\n",
    "        self,\n",
    "        client,\n",
    "        embedding_model_name: str = \"all-MiniLM-L6-v2\",\n",
    "        cross_encoder_model_name: str = \"cross-encoder/ms-marco-MiniLM-L-6-v2\"\n",
    "    ):\n",
    "        self.embedder = SentenceTransformer(embedding_model_name)\n",
    "        self.reranker = CrossEncoder(cross_encoder_model_name)\n",
    "        self.cache = Cache(CACHE_PATH)\n",
    "\n",
    "        self.client = client\n",
    "        self.collection = self.client.get_or_create_collection(name=\"email_chunks\")\n",
    "\n",
    "    def embed_query(self, query: str) -> List[float]:\n",
    "        return self.embedder.encode(query).tolist()\n",
    "\n",
    "    def search(self, query: str, top_k: int = 5, filter_thread_id: int = None) -> List[Dict]:\n",
    "        query_hash = hashlib.md5(query.encode()).hexdigest()\n",
    "\n",
    "        if self.cache.contains(query_hash):\n",
    "            return self.cache.get(query_hash)\n",
    "\n",
    "        query_embedding = self.embed_query(query)\n",
    "\n",
    "        search_args = {\n",
    "                            \"query_embeddings\": [query_embedding],\n",
    "                            \"n_results\": top_k * 2,  # get more to allow for reranking\n",
    "                      }\n",
    "        \n",
    "        if filter_thread_id is not None:\n",
    "            search_args[\"where\"] = {\"thread_id\": int(filter_thread_id)}\n",
    "\n",
    "        results = self.collection.query(**search_args)\n",
    "\n",
    "        documents = results[\"documents\"][0]\n",
    "        metadatas = results[\"metadatas\"][0]\n",
    "\n",
    "        # === Re-ranking ===\n",
    "        pairs = [(query, doc) for doc in documents]\n",
    "        scores = self.reranker.predict(pairs)\n",
    "\n",
    "        reranked = sorted(zip(documents, metadatas, scores), key=lambda x: x[2], reverse=True)\n",
    "\n",
    "        top_chunks = [\n",
    "            {\"chunk\": doc, \"metadata\": meta, \"score\": score}\n",
    "            for doc, meta, score in reranked[:top_k]\n",
    "        ]\n",
    "\n",
    "        scored_chunks = convert_np_types(top_chunks)\n",
    "\n",
    "        self.cache.set(query_hash, scored_chunks)\n",
    "        return top_chunks\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b53bcc8-f41f-4401-b99b-9205f1d73dcc",
   "metadata": {},
   "source": [
    "## Generation Layer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ee0af25-fa4f-4f31-b14c-e465d93d408f",
   "metadata": {},
   "source": [
    "<img src=\"generation_layer_flow.jpg\" style=\"display:block; margin-left:auto; margin-right:auto;\"/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "1bf02612-c045-4b86-b53a-1124355b60c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# src/generation_layer.py\n",
    "\n",
    "import openai\n",
    "from typing import List, Dict\n",
    "import os\n",
    "\n",
    "# Set your OpenAI API key securely\n",
    "openai.api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "\n",
    "# === Prompt Template ===\n",
    "\n",
    "def build_prompt(query: str, chunks: List[Dict], few_shot: bool = False) -> str:\n",
    "    prompt = \"You are an assistant that summarizes and extracts insights from corporate email threads.\\n\"\n",
    "    prompt += \"Given a user query and the relevant email thread excerpts, answer the question concisely and accurately. answer the question based on what is explicitly stated in the emails\\n\\n\"\n",
    "\n",
    "    if few_shot:\n",
    "        prompt += (\n",
    "            \"Example:\\n\"\n",
    "            \"Query: What was the decision on the marketing budget for Q2?\\n\"\n",
    "            \"Context:\\n\"\n",
    "            \"- The marketing team proposed a 20% increase for digital campaigns.\\n\"\n",
    "            \"- Finance approved a 10% increase after negotiation.\\n\"\n",
    "            \"Answer: A 10% increase in the Q2 marketing budget was approved after negotiation.\\n\\n\"\n",
    "        )\n",
    "\n",
    "    prompt += f\"Query: {query}\\nContext:\\n\"\n",
    "    for idx, chunk in enumerate(chunks):\n",
    "        prompt += f\"- {chunk['chunk']}\\n\"\n",
    "    prompt += \"\\nAnswer:\"\n",
    "    return prompt\n",
    "\n",
    "\n",
    "# === Generator Function ===\n",
    "\n",
    "def generate_answer(query: str, chunks: List[Dict], model: str = \"gpt-3.5-turbo\") -> str:\n",
    "    prompt = build_prompt(query, chunks, few_shot=True)\n",
    "\n",
    "    try:\n",
    "        response = client.chat.completions.create(\n",
    "            model=model,\n",
    "            messages=[\n",
    "                {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "                {\"role\": \"user\", \"content\": prompt}\n",
    "            ],\n",
    "            temperature=0.3,\n",
    "            max_tokens=300\n",
    "        )\n",
    "        answer = response.choices[0].message.content.strip()\n",
    "        return answer\n",
    "    except Exception as e:\n",
    "        print(f\"Error calling OpenAI API: {e}\")\n",
    "        return \"Sorry, I couldn't generate a response due to an error.\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "b7ff9482-b270-4f82-9cb6-0da7b05815b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading dataset...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to send telemetry event ClientStartEvent: capture() takes 1 positional argument but 3 were given\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedding data...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to send telemetry event ClientCreateCollectionEvent: capture() takes 1 positional argument but 3 were given\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedding 24987 chunks...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|██████████| 781/781 [30:01<00:00,  2.31s/it]  \n",
      "Failed to send telemetry event CollectionAddEvent: capture() takes 1 positional argument but 3 were given\n",
      "Failed to send telemetry event ClientCreateCollectionEvent: capture() takes 1 positional argument but 3 were given\n"
     ]
    }
   ],
   "source": [
    "# main.py\n",
    "\n",
    "# === CONFIG ===\n",
    "DATA_PATH = \"email_dataset/email_thread_details.csv\"\n",
    "TOP_K = 3\n",
    "\n",
    "# === LOAD DATA ===\n",
    "print(\"Loading dataset...\")\n",
    "df = pd.read_csv(DATA_PATH).dropna(subset=[\"body\"])\n",
    "\n",
    "chroma_client = chromadb.Client(Settings(persist_directory=\"chroma_db\"))\n",
    "\n",
    "# === EMBEDDING PHASE ===\n",
    "print(\"Embedding data...\")\n",
    "embedder = EmbeddingProcessor(client=chroma_client)\n",
    "embedder.process_emails(df)\n",
    "\n",
    "# === SEARCH PHASE ===\n",
    "search_engine = SearchEngine(client=chroma_client)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "177a8ceb-7230-4f26-824d-356c47f378a5",
   "metadata": {},
   "source": [
    "### Query wise execution & generate results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6d05085-8941-4a32-9bb2-3a416568637a",
   "metadata": {},
   "source": [
    "### Self designed Queries\n",
    "\n",
    "Here are the 3 required queries:\n",
    "\n",
    "1. What is the agenda of the Credit Group Lunch on May 5th?\n",
    "\n",
    "2. Which golf courses were mentioned as potential venues?\n",
    "\n",
    "3. Who generated and sent the manual invoice to Southwest Gas, and when?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5856201c-4ee8-4f8e-8015-984268e170ae",
   "metadata": {},
   "source": [
    "### Query1 : What is the agenda of the Credit Group Lunch on May 5th?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "bcfea6b1-4b1d-4dd9-aaaf-2bb2ffb5d746",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== QUERY: What is the agenda of the Credit Group Lunch on May 5th? ===\n",
      "\n",
      "Top Retrieved Chunks:\n",
      "\n",
      "--- Chunk 1 ---\n",
      "Gosh, I guessed right!!!! Kaye Ellis 04/18/2000 01:51 PM To: Sara Shackleton/HOU/ECT@ECT cc: Subject: Re: Credit Group Lunch Jeff Sorenson would like the meeting on May 12 to be from 11:30a to 1p.\n",
      "Metadata: {'from': 'Sara Shackleton', 'subject': 'Credit Group Lunch', 'thread_id': 2, 'timestamp': '2000-04-18 08:29:00'}\n",
      "\n",
      "--- Chunk 2 ---\n",
      "Suzanne: Here is the complete list of credit folks. Please send an e-mail to each of them concerning the 5th. Please include the description that I have bolded. In our group, you don't need to include Marie or Shari. Thanks. Carol ---------------------- Forwarded by Carol St Clair/HOU/ECT on 04/18/2000 11:52 AM --------------------------- From: John Suttle 04/18/2000 11:47 AM To: Carol St Clair/HOU/ECT@ECT cc: Subject: Re: Credit Group Lunch Carol, Three more have recently joined our group: Ed Sacks Brad Schneider Wendy LeBrocq JS Carol St Clair 04/18/2000 11:43 AM To: John Suttle/HOU/ECT@ECT cc: Subject: Credit Group Lunch John: Sara and I would like to hold another lunch with your group on Friday, May 5th to go through in detail how the ISDA and CSA Masters and Schedules work. Could you please take a look at this list and let me know of any additions or deletions? Thanks. Carol Bill Bradford Debbie Brackett Tanya Rohauer Rod Nelson Russell Diamond Veronica Espinoza Tracy Ngo Brant Reves Kevin Radous Tom Moran Christopher Smith Lesli Campbell Cathy Tudon Nidia Martinez Molly Harris Thanks. Carol\n",
      "Metadata: {'from': 'Carol St Clair', 'subject': 'Credit Group Lunch', 'thread_id': 2, 'timestamp': '2000-04-18 04:54:00'}\n",
      "\n",
      "--- Chunk 3 ---\n",
      "Sara and Tana: FYI. Please when you know it provide Suzanne with the headcount for the confirm/settlements program on May 5th and the global contracts program on May 24th and I will take care of the credit program on May 12th. Carol ---------------------- Forwarded by Carol St Clair/HOU/ECT on 04/13/2000 03:22 PM --------------------------- Suzanne Adams 04/13/2000 12:05 PM To: Carol St Clair/HOU/ECT@ECT cc: Subject: Re: Conference Rooms All reserved from 11:30 a.m.-2:00 p.m. May 5: 30C2 May 12: 30C2 May 24: 46C1 June 16: 30C2 Please let me know how many people will be attending each meeting so I can order lunch. Carol St Clair 04/13/2000 11:48 AM To: Suzanne Adams/HOU/ECT@ECT cc: Subject: Conference Rooms Suzanne: Just wanted to confirm before we sent out any invitations that we have the following rooms reserved: Friday, May 5th 11:30-2 30th floor Friday May 12th 11:30-2 30th Floor Wednesday May 24th 11:30-2 Which room did we get? Friday, June 16th 11:30-2 30th Floor Carol\n",
      "Metadata: {'from': 'Carol St Clair', 'subject': 'Conference Rooms', 'thread_id': 212, 'timestamp': '2000-04-13 08:25:00'}\n"
     ]
    }
   ],
   "source": [
    "query1 = \"What is the agenda of the Credit Group Lunch on May 5th?\"\n",
    "TOP_K = 3\n",
    "\n",
    "print(f\"\\n=== QUERY: {query1} ===\")\n",
    "\n",
    "# Search Layer outputs\n",
    "top_chunks1 = search_engine.search(query1, top_k=TOP_K)\n",
    "\n",
    "# Print top chunks\n",
    "print(\"\\nTop Retrieved Chunks:\")\n",
    "for i, chunk in enumerate(top_chunks1):\n",
    "    print(f\"\\n--- Chunk {i+1} ---\")\n",
    "    print(f\"{chunk['chunk']}\")\n",
    "    print(f\"Metadata: {chunk['metadata']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44f4a963-7f30-4bc9-b19d-faca34b4b047",
   "metadata": {},
   "source": [
    "### Query1 : What is the agenda of the Credit Group Lunch on May 5th?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "96106d14-2a8e-411f-b3b8-cd7cf0253db0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Generated Layer Output:\n",
      "The agenda of the Credit Group Lunch on May 5th is to go through in detail how the ISDA and CSA Masters and Schedules work.\n"
     ]
    }
   ],
   "source": [
    "# Generatove layer Outputs\n",
    "open_ai_output1 = generate_answer(query1, top_chunks1)\n",
    "print(\"\\nGenerated Layer Output:\")\n",
    "print(open_ai_output1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b23165dd-de9b-4518-879b-a5d82ca9ad78",
   "metadata": {},
   "source": [
    "### Query2 : Which golf courses were mentioned as potential venues?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "c9eb889e-4592-4b14-a464-497c11cedf18",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== QUERY: Which golf courses were mentioned as potential venues? ===\n",
      "\n",
      "Top Retrieved Chunks:\n",
      "\n",
      "--- Chunk 1 ---\n",
      "Doug, Sounds fun but I can't commit now. Please do not wait on me, go ahead and fill up the foursome. thanks,mike From: Doug Leach 09/29/2000 10:01 AM To: Mike McConnell/HOU/ECT@ECT cc: Randal T Maffett/HOU/ECT@ECT, Tom Briggs/NA/Enron@Enron Subject: golf Mike, Can you join us on Wednesday morning, November 15 to play golf at Canyon Springs golf course in San Antonio prior to the Enron Management Conference? Canyon Springs will allow us to book a 10:00am tee time thirty days in advance. Once I have confirmed the tee time I will forward directions to the course. Should be a fun group. Might even need a practice round at Champions prior to the trip. Doug\n",
      "Metadata: {'from': 'Mike McConnell', 'subject': 'golf', 'thread_id': 1884, 'timestamp': '2000-10-02 10:47:00'}\n",
      "\n",
      "--- Chunk 2 ---\n",
      "Call me if you wanted anything else. ---------------------- Forwarded by Brad Guilmino/HOU/EES on 09/14/2001 10:08 AM --------------------------- From: Ryan O'Rourke/ENRON@enronXgate on 09/14/2001 10:03 AM To: Brad Guilmino/HOU/EES@EES cc: Subject: RE: golf Pinehurst- $35 Clear Creek- $27 Hermann Park- $27 Magnolia Creek (League City)- $45 These are all twilight rates (ie. mid afternoon and on). Magnolia Creek is a lot of fun I think, if you're willing to pay the price and willing to make the drive. It's nice, long, new... it's a European style links course. -----Original Message----- From: Guilmino, Brad Sent: Friday, September 14, 2001 9:55 AM To: O'Rourke, Ryan Subject: golf Ryan, My roomate was asking me what golf courses we play besides Memorial. Can you email a few with price estimates?\n",
      "Metadata: {'from': 'Guilmino, Brad NOTESADDR/CN=36255EEC-43629EB9-862569DE-7C0715', 'subject': 'RE: golf', 'thread_id': 4107, 'timestamp': '2001-09-14 08:09:01'}\n",
      "\n",
      "--- Chunk 3 ---\n",
      "Will do....have a great game! \"Gann, Christopher (CE)\" <CEGANN@dow.com> on 05/24/2001 01:39:21 PM To: \"'Stanley.Horton@enron.com'\" <Stanley.Horton@enron.com> cc: Subject: RE: Sunday Golf at Rav Cindy, Thank you...please alert Stan that I will be there. Regards, Chris -----Original Message----- From: Stanley.Horton@enron.com [mailto:Stanley.Horton@enron.com] Sent: Thursday, May 24, 2001 1:20 PM To: DFS International, Inc.; CEGANN@dow.com Subject: Re: Sunday Golf at Rav Importance: High Hello! Stan asked that I advise you of Sunday's (May 27) golf details at Raveneaux Country Club, as follows: Stan Horton Chris Gann Steve Westlund Julie Westlund Tee time is 8:15AM on the Old Course off #1. Don't hesitate to call me if you have any questions or concerns. Thanks, Cindy (Stan's assistant) 713/853-6197 \"DFS International, Inc.\" <dfsintl@swbell.net> on 05/23/2001 11:09:14 AM To: stanley.horton@enron.com cc: cegann@dow.com Subject: Sunday Golf at Rav After negotiations with my better half, I will be ready to play Sunday. My wife Julie is also willing to play on Sunday and we are trying to arrange for a baby sitter. Either way, I will be there. Please let me know the time when it comes available. Thank you. Best regards. Steve Westlund\n",
      "Metadata: {'from': 'Cindy Stark', 'subject': 'RE: Sunday Golf at Rav', 'thread_id': 3272, 'timestamp': '2001-05-24 07:53:00'}\n"
     ]
    }
   ],
   "source": [
    "query2 = \"Which golf courses were mentioned as potential venues?\"\n",
    "TOP_K = 3\n",
    "print(f\"\\n=== QUERY: {query2} ===\")\n",
    "# Search Layer outputs\n",
    "top_chunks2 = search_engine.search(query2, top_k=TOP_K)\n",
    "# Print top chunks\n",
    "print(\"\\nTop Retrieved Chunks:\")\n",
    "for i, chunk in enumerate(top_chunks2):\n",
    "    print(f\"\\n--- Chunk {i+1} ---\")\n",
    "    print(f\"{chunk['chunk']}\")\n",
    "    print(f\"Metadata: {chunk['metadata']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfa8d15a-1eb8-4cfd-93c5-df26efa8964e",
   "metadata": {},
   "source": [
    "### Query2 : Which golf courses were mentioned as potential venues?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "2f803925-f4d8-4b33-88ae-0374c78f8af1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Generated Layer Output:\n",
      "Query: Which golf courses were mentioned as potential venues?\n",
      "Context:\n",
      "- Doug Leach invited Mike to play golf at Canyon Springs golf course in San Antonio.\n",
      "- Ryan O'Rourke mentioned Pinehurst, Clear Creek, Hermann Park, and Magnolia Creek as potential golf courses.\n",
      "- Chris Gann confirmed playing at Raveneaux Country Club.\n",
      "Answer: Potential golf venues mentioned include Canyon Springs, Pinehurst, Clear Creek, Hermann Park, Magnolia Creek, and Raveneaux Country Club.\n"
     ]
    }
   ],
   "source": [
    "# Generatove layer Outputs\n",
    "open_ai_output2 = generate_answer(query2, top_chunks2)\n",
    "print(\"\\nGenerated Layer Output:\")\n",
    "print(open_ai_output2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2d7f937-605c-4d7b-99aa-bdac3e444d3f",
   "metadata": {},
   "source": [
    "### Query3 : Who generated and sent the manual invoice to Southwest Gas, and when?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "d7199c50-4e01-405b-9ec1-03dfaa922be9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== QUERY: Who generated and sent the manual invoice to Southwest Gas, and when? ===\n",
      "\n",
      "Top Retrieved Chunks:\n",
      "\n",
      "--- Chunk 1 ---\n",
      "Would you see if we sent out invoices our if someone at Enron requested that no invoices be sent out. Thanks -----Original Message----- From: Dhont, Margaret Sent: Friday, March 22, 2002 2:04 PM To: Germany, Chris Subject: RE: Letter re Unpaid Invoice for Post petition Deliveries Chris We were not paid by either cornerstone Propane or Midamerican for these deliveries. Margaret -----Original Message----- From: Germany, Chris Sent: Thursday, March 14, 2002 6:32 PM To: Dhont, Margaret; Wynne, Rita; Chance, Lee Ann Cc: Olinger, Kimberly S.; Concannon, Ruth Subject: RE: Letter re Unpaid Invoice for Post petition Deliveries I had a letter that needed to be sent out today so I left early. This is what I need and you can tell me what the process is. ENA purchased gas from TDC (sitara #1143983) in the month of December 2001 on NGPL. It appears that we scheduled the gas on an NGPL IT agreement to the NGPL LA Pool. At the pool, it appears that we made some sales Deal 1184526 Cornerstone Propane, L.P. Deal 1184587 MidAmerican Energy Company Deals 258085 and 315861 to Enron MW, L.L.C. - desk to desk deals. Did Cornersone pay ENA for the gas? Did MidAmerican pay ENA for the gas? How do these desk to desk deals work? Did any of this goes go to cash out or did ENA go short and cash out? Not a rush but I would like to have an answer by next Friday if possible. Thanks -----Original Message----- From: McMichael Jr., Ed Sent: Monday, March 11, 2002 8:26 PM To: Germany, Chris Cc: Mann, Kay; Dicarlo, Louis; Dhont, Margaret; Polsky, Phil; Boyt, Eric; Parks, Joe; 'Melanie Gray (E-mail)' Subject: RE: Letter re Unpaid Invoice for Post petition Deliveries Please work with Energy Operations to determine where it went and if we sole it or if is still on the pipe. The priority should be based on your judgment. Ed -----Original Message----- From: Germany, Chris Sent: Monday, March 11, 2002 4:38 PM To: McMichael Jr., Ed Cc: Mann, Kay; McMichael Jr., Ed; Dicarlo, Louis; Dhont, Margaret; Polsky, Phil; Boyt, Eric; Parks, Joe; Melanie Gray (E-mail) Subject: RE: Letter re Unpaid Invoice for Post petition Deliveries Ed, this gas was nominated on an IT contract into a pool on NGPL. I am unable to identify the gas after that so its difficult for me to tell if the estate benefited. Let me know what you want to do at this point. Thanks -----Original Message----- From: melanie.gray@weil.com@ENRON Sent: Wednesday, March 06, 2002 3:30 PM To: Germany, Chris Cc: Mann, Kay; McMichael Jr., Ed; Dicarlo, Louis; Dhont, Margaret; Polsky, Phil; Boyt, Eric; Parks, Joe Subject: RE: Letter re Unpaid Invoice for Post petition Deliveries Assuming all of what is below is all correct, it appears that TDC is entitled to payment. The question with administrative priority is whether the the estate benefitted. If we took the gas, I assume that we did. THanks. \"Germany, Chris\" <Chris.Germany@ENRON.com> on 03/06/2002 02:38:33 PM cc: \"Dicarlo, Louis\" <Louis.Dicarlo@ENRON.com>, \"Dhont, Margaret\" <Margaret.Dhont@ENRON.com>, \"Polsky, Phil\" <Philip.Polsky@ENRON.com>, \"Boyt, Eric\" <Eric.Boyt@ENRON.com>, \"Parks, Joe\"\n",
      "Metadata: {'from': 'Germany, Chris CGERMAN', 'subject': 'RE: Letter re Unpaid Invoice for Post petition Deliveries', 'thread_id': 2133, 'timestamp': '2002-03-29 12:49:39'}\n",
      "\n",
      "--- Chunk 2 ---\n",
      "Attached are the final documents which includes 1. Agency and Management Agreement - Clean and Redlined copy which shows the changes per my discussion with Jay Goleb at Baker & Botts 2. Gas Confirm for June 1 thru June 4, 2001 3. Gas Confirm for June 5 thru June 30, 2001 4. Gas Conform for July 1 thru October 31, 2001 5. GTC which applies to all three confirms 6. Agency Notice Letter - Mexicana to send to its Permian suppliers for notice of Enron's agency. Please print out and execute documents 1 thru 5 and fax to Barry Tycholiz. We will follow up with duplicate originals for execution. Thanks for all your cooperation.\n",
      "Metadata: {'from': 'Gerald Nemec', 'subject': 'Final Docs.', 'thread_id': 1048, 'timestamp': '2001-06-01 19:22:00'}\n",
      "\n",
      "--- Chunk 3 ---\n",
      "Attached are the final documents which includes 1. Agency and Management Agreement - Clean and Redlined copy which shows the changes per my discussion with Jay Goleb at Baker & Botts 2. Gas Confirm for June 1 thru June 4, 2001 3. Gas Confirm for June 5 thru June 30, 2001 4. Gas Conform for July 1 thru October 31, 2001 5. GTC which applies to all three confirms 6. Agency Notice Letter - Mexicana to send to its Permian suppliers for notice of Enron's agency. Please print out and execute documents 1 thru 5 and fax to Barry Tycholiz. We will follow up with duplicate originals for execution. Thanks for all your cooperation.\n",
      "Metadata: {'from': 'Gerald Nemec', 'subject': 'Final Docs.', 'thread_id': 1048, 'timestamp': '2001-06-01 09:22:00'}\n"
     ]
    }
   ],
   "source": [
    "query3 = \"Who generated and sent the manual invoice to Southwest Gas, and when?\"\n",
    "TOP_K = 3\n",
    "print(f\"\\n=== QUERY: {query3} ===\")\n",
    "# Search Layer outputs\n",
    "top_chunks3 = search_engine.search(query3, top_k=TOP_K)\n",
    "# Print top chunks\n",
    "print(\"\\nTop Retrieved Chunks:\")\n",
    "for i, chunk in enumerate(top_chunks3):\n",
    "    print(f\"\\n--- Chunk {i+1} ---\")\n",
    "    print(f\"{chunk['chunk']}\")\n",
    "    print(f\"Metadata: {chunk['metadata']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55cb07f5-f3a2-4ca4-8f64-67dfc949ffdc",
   "metadata": {},
   "source": [
    "### Query3 : Who generated and sent the manual invoice to Southwest Gas, and when?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "de0060a9-2923-4250-9f68-a304df06ecc3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Generated Layer Output:\n",
      "The manual invoice to Southwest Gas was generated and sent by Chris Germany. The specific date is not mentioned in the email thread.\n"
     ]
    }
   ],
   "source": [
    "# Generatove layer Outputs\n",
    "open_ai_output3 = generate_answer(query3, top_chunks3)\n",
    "print(\"\\nGenerated Layer Output:\")\n",
    "print(open_ai_output3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ead0875-2b93-416e-9a32-2bfdc38c3c14",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0f87e55-ce8d-413f-b6d9-36423c5d9ec5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c6b0a6f1-6997-4151-90f8-2051d602ba77",
   "metadata": {},
   "source": [
    "## Batch Evaluator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "6bdaeb81-e8eb-4533-894d-d2e1425a7c49",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:13<00:00,  1.36s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch evaluation complete. Results saved to `batch_evaluation_results.csv`.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from evaluate import load\n",
    "#from src.search_layer import SearchEngine\n",
    "#from src.generation_layer import generate_answer\n",
    "\n",
    "# Load ROUGE metric\n",
    "rouge = load(\"rouge\")\n",
    "\n",
    "# Load datasets\n",
    "threads_df = pd.read_csv(\"email_dataset/email_thread_details.csv\")\n",
    "summaries_df = pd.read_csv(\"email_dataset/email_thread_summaries.csv\")\n",
    "\n",
    "\n",
    "# Number of samples to test (limit for speed, e.g., 10)\n",
    "N = 10\n",
    "\n",
    "# Collect results\n",
    "results = []\n",
    "\n",
    "for i in tqdm(range(N)):\n",
    "    row = summaries_df.iloc[i]\n",
    "    thread_id = row['thread_id']\n",
    "    reference_summary = row['summary']\n",
    "\n",
    "    # Get full email body for the thread\n",
    "    thread_emails = threads_df[threads_df['thread_id'] == thread_id]\n",
    "    email_texts = thread_emails['body'].tolist()\n",
    "\n",
    "    if not email_texts:\n",
    "        continue\n",
    "\n",
    "    query = \"Summarize the key decisions made in this email thread.\"\n",
    "    # Perform search on chunks\n",
    "    top_chunks = search_engine.search(query, top_k=3, filter_thread_id=thread_id)\n",
    "\n",
    "    # If no chunks found, skip\n",
    "    if not top_chunks:\n",
    "        continue\n",
    "\n",
    "    # Generate answer from chunks\n",
    "    try:\n",
    "        generated_summary = generate_answer(query, top_chunks)\n",
    "    except Exception as e:\n",
    "        print(f\"Error generating summary: {e}\")\n",
    "        continue\n",
    "\n",
    "    # Compute ROUGE scores\n",
    "    rouge_scores = rouge.compute(predictions=[generated_summary], references=[reference_summary])\n",
    "\n",
    "    results.append({\n",
    "        \"thread_id\": thread_id,\n",
    "        \"query\": query,\n",
    "        \"reference_summary\": reference_summary,\n",
    "        \"generated_summary\": generated_summary,\n",
    "        \"rouge1\": rouge_scores[\"rouge1\"],\n",
    "        \"rouge2\": rouge_scores[\"rouge2\"],\n",
    "        \"rougeL\": rouge_scores[\"rougeL\"]\n",
    "    })\n",
    "\n",
    "# Save to CSV\n",
    "results_df = pd.DataFrame(results)\n",
    "results_df.to_csv(\"batch_evaluation_results.csv\", index=False)\n",
    "\n",
    "print(\"Batch evaluation complete. Results saved to `batch_evaluation_results.csv`.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9edc0ba1-94ed-4acf-b5c6-1f56e88c02f1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "9db84971-56fc-4680-ac11-730566ae5280",
   "metadata": {},
   "source": [
    "## Future Enhancements"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "991ce0ef-99a4-4378-af07-733ebcf39843",
   "metadata": {},
   "source": [
    "**Embedding Layer Enhancements:**\n",
    "\n",
    "- Parallelize or batch chunking and embedding for large datasets.\n",
    "\n",
    "- Support multilingual email embedding using a multilingual transformer model (e.g., distiluse-base-multilingual-cased).\n",
    "\n",
    "- Add logging and error handling during embedding and chunking.\n",
    "\n",
    "- Deduplicate similar chunks before storing in the vector DB to reduce redundancy.\n",
    "\n",
    "- Store additional metadata (e.g., department, priority) to enable advanced filtering during search."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4409b692-3774-44c5-a11b-18ee19e319e7",
   "metadata": {},
   "source": [
    "**Cahce Layer Enhancements:**\n",
    "\n",
    "- Replace JSON with Redis or SQLite for faster lookup and persistence in multi-user environments.\n",
    "\n",
    "- Add cache eviction policy (e.g., LRU) to avoid unlimited growth.\n",
    "\n",
    "- Track cache hit/miss stats for performance analytics.\n",
    "\n",
    "- Encrypt cache contents if storing sensitive queries or responses."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba40cebb-df47-43dd-bad0-7ebfe03d5a07",
   "metadata": {},
   "source": [
    "**Search Layer Enhancements:**\n",
    "\n",
    "- Improve reranking with better cross-encoders like bge-reranker or cohere models.\n",
    "\n",
    "- Add semantic filters beyond thread_id (e.g., date, sender, topic).\n",
    "\n",
    "- Support multi-query or follow-up query handling (e.g., thread-based QA).\n",
    "\n",
    "- Paginate results and allow sorting based on relevance, timestamp, etc.\n",
    "\n",
    "- Expose the search as an API with configurable parameters."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43260ebe-cd16-448f-a618-92d406e88455",
   "metadata": {},
   "source": [
    "**Generation Layer Enhancements:**\n",
    "\n",
    "- Use function calling / structured output instead of plain text (for automation).\n",
    "\n",
    "- Support custom prompt templates per use case (summarization, classification, etc.).\n",
    "\n",
    "- Switch to a self-hosted model (e.g., LLaMA 3, Mistral) for cost and privacy control.\n",
    "\n",
    "- Limit token count dynamically to avoid truncation of large prompts.\n",
    "\n",
    "- Stream responses if using GPT-4-turbo for better UX."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e188ca78-36f6-4946-9a7f-70aff1635b28",
   "metadata": {},
   "source": [
    "**Overall Architecture Enhancements**\n",
    "\n",
    "- Centralized logging and monitoring (e.g., using logging, Sentry, or Prometheus).\n",
    "\n",
    "- Unit and integration tests for all layers to ensure robustness.\n",
    "\n",
    "- Add retry mechanisms for external API calls (OpenAI, Chroma).\n",
    "\n",
    "- Implement role-based access control (RBAC) if deployed in an enterprise environment.\n",
    "\n",
    "- Deploy as a containerized microservice (Docker + FastAPI) with endpoints for embedding, search, and generation.\n",
    "\n",
    "- Add a front-end interface for uploading emails, searching threads, and viewing generated insights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6f5d6bd-f845-4bf9-8b91-21799567ff0e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
