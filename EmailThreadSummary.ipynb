{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "587dd5ed-3e70-4113-90b3-8c35649f68f9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "db4fb8c1-e9cf-4c4f-b3be-15326f8101e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Admin\\anaconda3\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "# Import the libraries\n",
    "import os\n",
    "import re\n",
    "import pandas as pd\n",
    "from typing import List, Dict\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import chromadb\n",
    "from chromadb.config import Settings\n",
    "from chromadb.utils import embedding_functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "7f5017b7-6c7e-4712-8c6a-49172d053321",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pandas==2.2.2 (from -r requirements.txt (line 1))"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Ignoring invalid distribution ~atplotlib (C:\\Users\\Admin\\anaconda3\\Lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution ~atplotlib (C:\\Users\\Admin\\anaconda3\\Lib\\site-packages)\n",
      "  WARNING: Failed to remove contents in a temporary directory 'C:\\Users\\Admin\\anaconda3\\Lib\\site-packages\\~-ndas.libs'.\n",
      "  You can safely remove it manually.\n",
      "  WARNING: Failed to remove contents in a temporary directory 'C:\\Users\\Admin\\anaconda3\\Lib\\site-packages\\~-ndas'.\n",
      "  You can safely remove it manually.\n",
      "WARNING: Ignoring invalid distribution ~atplotlib (C:\\Users\\Admin\\anaconda3\\Lib\\site-packages)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "  Using cached pandas-2.2.2-cp312-cp312-win_amd64.whl.metadata (19 kB)\n",
      "Requirement already satisfied: numpy==1.26.4 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from -r requirements.txt (line 2)) (1.26.4)\n",
      "Collecting tqdm==4.66.4 (from -r requirements.txt (line 3))\n",
      "  Using cached tqdm-4.66.4-py3-none-any.whl.metadata (57 kB)\n",
      "Collecting scikit-learn==1.5.0 (from -r requirements.txt (line 4))\n",
      "  Using cached scikit_learn-1.5.0-cp312-cp312-win_amd64.whl.metadata (11 kB)\n",
      "Collecting sentence-transformers==2.6.1 (from -r requirements.txt (line 7))\n",
      "  Using cached sentence_transformers-2.6.1-py3-none-any.whl.metadata (11 kB)\n",
      "Collecting transformers==4.42.1 (from -r requirements.txt (line 8))\n",
      "  Using cached transformers-4.42.1-py3-none-any.whl.metadata (43 kB)\n",
      "Collecting chromadb==0.5.0 (from -r requirements.txt (line 11))\n",
      "  Using cached chromadb-0.5.0-py3-none-any.whl.metadata (7.3 kB)\n",
      "Requirement already satisfied: openai==1.95.1 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from -r requirements.txt (line 14)) (1.95.1)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\admin\\appdata\\roaming\\python\\python312\\site-packages (from pandas==2.2.2->-r requirements.txt (line 1)) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from pandas==2.2.2->-r requirements.txt (line 1)) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from pandas==2.2.2->-r requirements.txt (line 1)) (2023.3)\n",
      "Requirement already satisfied: colorama in c:\\users\\admin\\anaconda3\\lib\\site-packages (from tqdm==4.66.4->-r requirements.txt (line 3)) (0.4.6)\n",
      "Requirement already satisfied: scipy>=1.6.0 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from scikit-learn==1.5.0->-r requirements.txt (line 4)) (1.13.1)\n",
      "Requirement already satisfied: joblib>=1.2.0 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from scikit-learn==1.5.0->-r requirements.txt (line 4)) (1.4.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from scikit-learn==1.5.0->-r requirements.txt (line 4)) (3.5.0)\n",
      "Requirement already satisfied: torch>=1.11.0 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from sentence-transformers==2.6.1->-r requirements.txt (line 7)) (2.5.1)\n",
      "Collecting huggingface-hub>=0.15.1 (from sentence-transformers==2.6.1->-r requirements.txt (line 7))\n",
      "  Using cached huggingface_hub-0.34.4-py3-none-any.whl.metadata (14 kB)\n",
      "Requirement already satisfied: Pillow in c:\\users\\admin\\appdata\\roaming\\python\\python312\\site-packages (from sentence-transformers==2.6.1->-r requirements.txt (line 7)) (11.1.0)\n",
      "Requirement already satisfied: filelock in c:\\users\\admin\\anaconda3\\lib\\site-packages (from transformers==4.42.1->-r requirements.txt (line 8)) (3.13.1)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\admin\\appdata\\roaming\\python\\python312\\site-packages (from transformers==4.42.1->-r requirements.txt (line 8)) (24.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from transformers==4.42.1->-r requirements.txt (line 8)) (6.0.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from transformers==4.42.1->-r requirements.txt (line 8)) (2024.11.6)\n",
      "Requirement already satisfied: requests in c:\\users\\admin\\anaconda3\\lib\\site-packages (from transformers==4.42.1->-r requirements.txt (line 8)) (2.32.3)\n",
      "Collecting tokenizers<0.20,>=0.19 (from transformers==4.42.1->-r requirements.txt (line 8))\n",
      "  Using cached tokenizers-0.19.1-cp312-none-win_amd64.whl.metadata (6.9 kB)\n",
      "Collecting safetensors>=0.4.1 (from transformers==4.42.1->-r requirements.txt (line 8))\n",
      "  Using cached safetensors-0.6.2-cp38-abi3-win_amd64.whl.metadata (4.1 kB)\n",
      "Collecting build>=1.0.3 (from chromadb==0.5.0->-r requirements.txt (line 11))\n",
      "  Using cached build-1.3.0-py3-none-any.whl.metadata (5.6 kB)\n",
      "Requirement already satisfied: pydantic>=1.9 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from chromadb==0.5.0->-r requirements.txt (line 11)) (2.10.5)\n",
      "Collecting chroma-hnswlib==0.7.3 (from chromadb==0.5.0->-r requirements.txt (line 11))\n",
      "  Using cached chroma-hnswlib-0.7.3.tar.gz (31 kB)\n",
      "  Installing build dependencies: started\n",
      "  Installing build dependencies: finished with status 'done'\n",
      "  Getting requirements to build wheel: started\n",
      "  Getting requirements to build wheel: finished with status 'done'\n",
      "  Preparing metadata (pyproject.toml): started\n",
      "  Preparing metadata (pyproject.toml): finished with status 'done'\n",
      "Collecting fastapi>=0.95.2 (from chromadb==0.5.0->-r requirements.txt (line 11))\n",
      "  Using cached fastapi-0.116.1-py3-none-any.whl.metadata (28 kB)\n",
      "Collecting uvicorn>=0.18.3 (from uvicorn[standard]>=0.18.3->chromadb==0.5.0->-r requirements.txt (line 11))\n",
      "  Using cached uvicorn-0.35.0-py3-none-any.whl.metadata (6.5 kB)\n",
      "Collecting posthog>=2.4.0 (from chromadb==0.5.0->-r requirements.txt (line 11))\n",
      "  Using cached posthog-6.6.1-py3-none-any.whl.metadata (6.0 kB)\n",
      "Requirement already satisfied: typing-extensions>=4.5.0 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from chromadb==0.5.0->-r requirements.txt (line 11)) (4.12.2)\n",
      "Collecting onnxruntime>=1.14.1 (from chromadb==0.5.0->-r requirements.txt (line 11))\n",
      "  Using cached onnxruntime-1.22.1-cp312-cp312-win_amd64.whl.metadata (5.1 kB)\n",
      "Collecting opentelemetry-api>=1.2.0 (from chromadb==0.5.0->-r requirements.txt (line 11))\n",
      "  Using cached opentelemetry_api-1.36.0-py3-none-any.whl.metadata (1.5 kB)\n",
      "Collecting opentelemetry-exporter-otlp-proto-grpc>=1.2.0 (from chromadb==0.5.0->-r requirements.txt (line 11))\n",
      "  Using cached opentelemetry_exporter_otlp_proto_grpc-1.36.0-py3-none-any.whl.metadata (2.4 kB)\n",
      "Collecting opentelemetry-instrumentation-fastapi>=0.41b0 (from chromadb==0.5.0->-r requirements.txt (line 11))\n",
      "  Using cached opentelemetry_instrumentation_fastapi-0.57b0-py3-none-any.whl.metadata (2.2 kB)\n",
      "Collecting opentelemetry-sdk>=1.2.0 (from chromadb==0.5.0->-r requirements.txt (line 11))\n",
      "  Using cached opentelemetry_sdk-1.36.0-py3-none-any.whl.metadata (1.5 kB)\n",
      "Collecting pypika>=0.48.9 (from chromadb==0.5.0->-r requirements.txt (line 11))\n",
      "  Using cached pypika-0.48.9-py2.py3-none-any.whl\n",
      "Requirement already satisfied: overrides>=7.3.1 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from chromadb==0.5.0->-r requirements.txt (line 11)) (7.4.0)\n",
      "Collecting importlib-resources (from chromadb==0.5.0->-r requirements.txt (line 11))\n",
      "  Using cached importlib_resources-6.5.2-py3-none-any.whl.metadata (3.9 kB)\n",
      "Requirement already satisfied: grpcio>=1.58.0 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from chromadb==0.5.0->-r requirements.txt (line 11)) (1.71.0)\n",
      "Collecting bcrypt>=4.0.1 (from chromadb==0.5.0->-r requirements.txt (line 11))\n",
      "  Using cached bcrypt-4.3.0-cp39-abi3-win_amd64.whl.metadata (10 kB)\n",
      "Requirement already satisfied: typer>=0.9.0 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from chromadb==0.5.0->-r requirements.txt (line 11)) (0.9.0)\n",
      "Collecting kubernetes>=28.1.0 (from chromadb==0.5.0->-r requirements.txt (line 11))\n",
      "  Using cached kubernetes-33.1.0-py2.py3-none-any.whl.metadata (1.7 kB)\n",
      "Collecting tenacity>=8.2.3 (from chromadb==0.5.0->-r requirements.txt (line 11))\n",
      "  Using cached tenacity-9.1.2-py3-none-any.whl.metadata (1.2 kB)\n",
      "Collecting mmh3>=4.0.1 (from chromadb==0.5.0->-r requirements.txt (line 11))\n",
      "  Using cached mmh3-5.2.0-cp312-cp312-win_amd64.whl.metadata (14 kB)\n",
      "Collecting orjson>=3.9.12 (from chromadb==0.5.0->-r requirements.txt (line 11))\n",
      "  Using cached orjson-3.11.2-cp312-cp312-win_amd64.whl.metadata (1.2 kB)\n",
      "Requirement already satisfied: anyio<5,>=3.5.0 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from openai==1.95.1->-r requirements.txt (line 14)) (4.6.2)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from openai==1.95.1->-r requirements.txt (line 14)) (1.9.0)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from openai==1.95.1->-r requirements.txt (line 14)) (0.27.0)\n",
      "Requirement already satisfied: jiter<1,>=0.4.0 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from openai==1.95.1->-r requirements.txt (line 14)) (0.10.0)\n",
      "Requirement already satisfied: sniffio in c:\\users\\admin\\anaconda3\\lib\\site-packages (from openai==1.95.1->-r requirements.txt (line 14)) (1.3.0)\n",
      "Requirement already satisfied: idna>=2.8 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from anyio<5,>=3.5.0->openai==1.95.1->-r requirements.txt (line 14)) (3.7)\n",
      "Collecting pyproject_hooks (from build>=1.0.3->chromadb==0.5.0->-r requirements.txt (line 11))\n",
      "  Using cached pyproject_hooks-1.2.0-py3-none-any.whl.metadata (1.3 kB)\n",
      "Collecting starlette<0.48.0,>=0.40.0 (from fastapi>=0.95.2->chromadb==0.5.0->-r requirements.txt (line 11))\n",
      "  Using cached starlette-0.47.2-py3-none-any.whl.metadata (6.2 kB)\n",
      "Requirement already satisfied: certifi in c:\\users\\admin\\anaconda3\\lib\\site-packages (from httpx<1,>=0.23.0->openai==1.95.1->-r requirements.txt (line 14)) (2025.1.31)\n",
      "Requirement already satisfied: httpcore==1.* in c:\\users\\admin\\anaconda3\\lib\\site-packages (from httpx<1,>=0.23.0->openai==1.95.1->-r requirements.txt (line 14)) (1.0.2)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai==1.95.1->-r requirements.txt (line 14)) (0.14.0)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from huggingface-hub>=0.15.1->sentence-transformers==2.6.1->-r requirements.txt (line 7)) (2024.12.0)\n",
      "Requirement already satisfied: six>=1.9.0 in c:\\users\\admin\\appdata\\roaming\\python\\python312\\site-packages (from kubernetes>=28.1.0->chromadb==0.5.0->-r requirements.txt (line 11)) (1.17.0)\n",
      "Collecting google-auth>=1.0.1 (from kubernetes>=28.1.0->chromadb==0.5.0->-r requirements.txt (line 11))\n",
      "  Using cached google_auth-2.40.3-py2.py3-none-any.whl.metadata (6.2 kB)\n",
      "Requirement already satisfied: websocket-client!=0.40.0,!=0.41.*,!=0.42.*,>=0.32.0 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from kubernetes>=28.1.0->chromadb==0.5.0->-r requirements.txt (line 11)) (1.8.0)\n",
      "Collecting requests-oauthlib (from kubernetes>=28.1.0->chromadb==0.5.0->-r requirements.txt (line 11))\n",
      "  Using cached requests_oauthlib-2.0.0-py2.py3-none-any.whl.metadata (11 kB)\n",
      "Collecting oauthlib>=3.2.2 (from kubernetes>=28.1.0->chromadb==0.5.0->-r requirements.txt (line 11))\n",
      "  Using cached oauthlib-3.3.1-py3-none-any.whl.metadata (7.9 kB)\n",
      "Requirement already satisfied: urllib3>=1.24.2 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from kubernetes>=28.1.0->chromadb==0.5.0->-r requirements.txt (line 11)) (2.3.0)\n",
      "Collecting durationpy>=0.7 (from kubernetes>=28.1.0->chromadb==0.5.0->-r requirements.txt (line 11))\n",
      "  Using cached durationpy-0.10-py3-none-any.whl.metadata (340 bytes)\n",
      "Collecting coloredlogs (from onnxruntime>=1.14.1->chromadb==0.5.0->-r requirements.txt (line 11))\n",
      "  Using cached coloredlogs-15.0.1-py2.py3-none-any.whl.metadata (12 kB)\n",
      "Requirement already satisfied: flatbuffers in c:\\users\\admin\\anaconda3\\lib\\site-packages (from onnxruntime>=1.14.1->chromadb==0.5.0->-r requirements.txt (line 11)) (25.2.10)\n",
      "Requirement already satisfied: protobuf in c:\\users\\admin\\anaconda3\\lib\\site-packages (from onnxruntime>=1.14.1->chromadb==0.5.0->-r requirements.txt (line 11)) (5.29.4)\n",
      "Requirement already satisfied: sympy in c:\\users\\admin\\anaconda3\\lib\\site-packages (from onnxruntime>=1.14.1->chromadb==0.5.0->-r requirements.txt (line 11)) (1.13.3)\n",
      "Collecting importlib-metadata<8.8.0,>=6.0 (from opentelemetry-api>=1.2.0->chromadb==0.5.0->-r requirements.txt (line 11))\n",
      "  Using cached importlib_metadata-8.7.0-py3-none-any.whl.metadata (4.8 kB)\n",
      "Collecting googleapis-common-protos~=1.57 (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb==0.5.0->-r requirements.txt (line 11))\n",
      "  Using cached googleapis_common_protos-1.70.0-py3-none-any.whl.metadata (9.3 kB)\n",
      "Collecting opentelemetry-exporter-otlp-proto-common==1.36.0 (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb==0.5.0->-r requirements.txt (line 11))\n",
      "  Using cached opentelemetry_exporter_otlp_proto_common-1.36.0-py3-none-any.whl.metadata (1.8 kB)\n",
      "Collecting opentelemetry-proto==1.36.0 (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb==0.5.0->-r requirements.txt (line 11))\n",
      "  Using cached opentelemetry_proto-1.36.0-py3-none-any.whl.metadata (2.3 kB)\n",
      "Collecting opentelemetry-instrumentation-asgi==0.57b0 (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb==0.5.0->-r requirements.txt (line 11))\n",
      "  Using cached opentelemetry_instrumentation_asgi-0.57b0-py3-none-any.whl.metadata (2.0 kB)\n",
      "Collecting opentelemetry-instrumentation==0.57b0 (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb==0.5.0->-r requirements.txt (line 11))\n",
      "  Using cached opentelemetry_instrumentation-0.57b0-py3-none-any.whl.metadata (6.7 kB)\n",
      "Collecting opentelemetry-semantic-conventions==0.57b0 (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb==0.5.0->-r requirements.txt (line 11))\n",
      "  Using cached opentelemetry_semantic_conventions-0.57b0-py3-none-any.whl.metadata (2.4 kB)\n",
      "Collecting opentelemetry-util-http==0.57b0 (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb==0.5.0->-r requirements.txt (line 11))\n",
      "  Using cached opentelemetry_util_http-0.57b0-py3-none-any.whl.metadata (2.6 kB)\n",
      "Requirement already satisfied: wrapt<2.0.0,>=1.0.0 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from opentelemetry-instrumentation==0.57b0->opentelemetry-instrumentation-fastapi>=0.41b0->chromadb==0.5.0->-r requirements.txt (line 11)) (1.17.2)\n",
      "Collecting asgiref~=3.0 (from opentelemetry-instrumentation-asgi==0.57b0->opentelemetry-instrumentation-fastapi>=0.41b0->chromadb==0.5.0->-r requirements.txt (line 11))\n",
      "  Using cached asgiref-3.9.1-py3-none-any.whl.metadata (9.3 kB)\n",
      "Collecting backoff>=1.10.0 (from posthog>=2.4.0->chromadb==0.5.0->-r requirements.txt (line 11))\n",
      "  Using cached backoff-2.2.1-py3-none-any.whl.metadata (14 kB)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from pydantic>=1.9->chromadb==0.5.0->-r requirements.txt (line 11)) (0.6.0)\n",
      "Requirement already satisfied: pydantic-core==2.27.2 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from pydantic>=1.9->chromadb==0.5.0->-r requirements.txt (line 11)) (2.27.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from requests->transformers==4.42.1->-r requirements.txt (line 8)) (3.3.2)\n",
      "Requirement already satisfied: setuptools in c:\\users\\admin\\anaconda3\\lib\\site-packages (from torch>=1.11.0->sentence-transformers==2.6.1->-r requirements.txt (line 7)) (75.8.0)\n",
      "Collecting sympy (from onnxruntime>=1.14.1->chromadb==0.5.0->-r requirements.txt (line 11))\n",
      "  Using cached sympy-1.13.1-py3-none-any.whl.metadata (12 kB)\n",
      "Requirement already satisfied: networkx in c:\\users\\admin\\anaconda3\\lib\\site-packages (from torch>=1.11.0->sentence-transformers==2.6.1->-r requirements.txt (line 7)) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from torch>=1.11.0->sentence-transformers==2.6.1->-r requirements.txt (line 7)) (3.1.6)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from sympy->onnxruntime>=1.14.1->chromadb==0.5.0->-r requirements.txt (line 11)) (1.3.0)\n",
      "Requirement already satisfied: click<9.0.0,>=7.1.1 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from typer>=0.9.0->chromadb==0.5.0->-r requirements.txt (line 11)) (8.1.7)\n",
      "Collecting httptools>=0.6.3 (from uvicorn[standard]>=0.18.3->chromadb==0.5.0->-r requirements.txt (line 11))\n",
      "  Using cached httptools-0.6.4-cp312-cp312-win_amd64.whl.metadata (3.7 kB)\n",
      "Requirement already satisfied: python-dotenv>=0.13 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from uvicorn[standard]>=0.18.3->chromadb==0.5.0->-r requirements.txt (line 11)) (0.21.0)\n",
      "Collecting watchfiles>=0.13 (from uvicorn[standard]>=0.18.3->chromadb==0.5.0->-r requirements.txt (line 11))\n",
      "  Using cached watchfiles-1.1.0-cp312-cp312-win_amd64.whl.metadata (5.0 kB)\n",
      "Collecting websockets>=10.4 (from uvicorn[standard]>=0.18.3->chromadb==0.5.0->-r requirements.txt (line 11))\n",
      "  Using cached websockets-15.0.1-cp312-cp312-win_amd64.whl.metadata (7.0 kB)\n",
      "Collecting cachetools<6.0,>=2.0.0 (from google-auth>=1.0.1->kubernetes>=28.1.0->chromadb==0.5.0->-r requirements.txt (line 11))\n",
      "  Using cached cachetools-5.5.2-py3-none-any.whl.metadata (5.4 kB)\n",
      "Collecting pyasn1-modules>=0.2.1 (from google-auth>=1.0.1->kubernetes>=28.1.0->chromadb==0.5.0->-r requirements.txt (line 11))\n",
      "  Using cached pyasn1_modules-0.4.2-py3-none-any.whl.metadata (3.5 kB)\n",
      "Collecting rsa<5,>=3.1.4 (from google-auth>=1.0.1->kubernetes>=28.1.0->chromadb==0.5.0->-r requirements.txt (line 11))\n",
      "  Using cached rsa-4.9.1-py3-none-any.whl.metadata (5.6 kB)\n",
      "Collecting zipp>=3.20 (from importlib-metadata<8.8.0,>=6.0->opentelemetry-api>=1.2.0->chromadb==0.5.0->-r requirements.txt (line 11))\n",
      "  Using cached zipp-3.23.0-py3-none-any.whl.metadata (3.6 kB)\n",
      "Collecting humanfriendly>=9.1 (from coloredlogs->onnxruntime>=1.14.1->chromadb==0.5.0->-r requirements.txt (line 11))\n",
      "  Using cached humanfriendly-10.0-py2.py3-none-any.whl.metadata (9.2 kB)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from jinja2->torch>=1.11.0->sentence-transformers==2.6.1->-r requirements.txt (line 7)) (3.0.2)\n",
      "Collecting pyreadline3 (from humanfriendly>=9.1->coloredlogs->onnxruntime>=1.14.1->chromadb==0.5.0->-r requirements.txt (line 11))\n",
      "  Using cached pyreadline3-3.5.4-py3-none-any.whl.metadata (4.7 kB)\n",
      "Collecting pyasn1<0.7.0,>=0.6.1 (from pyasn1-modules>=0.2.1->google-auth>=1.0.1->kubernetes>=28.1.0->chromadb==0.5.0->-r requirements.txt (line 11))\n",
      "  Using cached pyasn1-0.6.1-py3-none-any.whl.metadata (8.4 kB)\n",
      "Using cached pandas-2.2.2-cp312-cp312-win_amd64.whl (11.5 MB)\n",
      "Using cached tqdm-4.66.4-py3-none-any.whl (78 kB)\n",
      "Using cached scikit_learn-1.5.0-cp312-cp312-win_amd64.whl (10.9 MB)\n",
      "Using cached sentence_transformers-2.6.1-py3-none-any.whl (163 kB)\n",
      "Using cached transformers-4.42.1-py3-none-any.whl (9.3 MB)\n",
      "Using cached chromadb-0.5.0-py3-none-any.whl (526 kB)\n",
      "Using cached bcrypt-4.3.0-cp39-abi3-win_amd64.whl (152 kB)\n",
      "Using cached build-1.3.0-py3-none-any.whl (23 kB)\n",
      "Using cached fastapi-0.116.1-py3-none-any.whl (95 kB)\n",
      "Using cached huggingface_hub-0.34.4-py3-none-any.whl (561 kB)\n",
      "Using cached kubernetes-33.1.0-py2.py3-none-any.whl (1.9 MB)\n",
      "Using cached mmh3-5.2.0-cp312-cp312-win_amd64.whl (41 kB)\n",
      "Using cached onnxruntime-1.22.1-cp312-cp312-win_amd64.whl (12.7 MB)\n",
      "Using cached opentelemetry_api-1.36.0-py3-none-any.whl (65 kB)\n",
      "Using cached opentelemetry_exporter_otlp_proto_grpc-1.36.0-py3-none-any.whl (18 kB)\n",
      "Using cached opentelemetry_exporter_otlp_proto_common-1.36.0-py3-none-any.whl (18 kB)\n",
      "Using cached opentelemetry_proto-1.36.0-py3-none-any.whl (72 kB)\n",
      "Using cached opentelemetry_instrumentation_fastapi-0.57b0-py3-none-any.whl (12 kB)\n",
      "Using cached opentelemetry_instrumentation-0.57b0-py3-none-any.whl (32 kB)\n",
      "Using cached opentelemetry_instrumentation_asgi-0.57b0-py3-none-any.whl (16 kB)\n",
      "Using cached opentelemetry_semantic_conventions-0.57b0-py3-none-any.whl (201 kB)\n",
      "Using cached opentelemetry_util_http-0.57b0-py3-none-any.whl (7.6 kB)\n",
      "Using cached opentelemetry_sdk-1.36.0-py3-none-any.whl (119 kB)\n",
      "Using cached orjson-3.11.2-cp312-cp312-win_amd64.whl (119 kB)\n",
      "Using cached posthog-6.6.1-py3-none-any.whl (119 kB)\n",
      "Using cached safetensors-0.6.2-cp38-abi3-win_amd64.whl (320 kB)\n",
      "Using cached tenacity-9.1.2-py3-none-any.whl (28 kB)\n",
      "Using cached tokenizers-0.19.1-cp312-none-win_amd64.whl (2.2 MB)\n",
      "Using cached sympy-1.13.1-py3-none-any.whl (6.2 MB)\n",
      "Using cached uvicorn-0.35.0-py3-none-any.whl (66 kB)\n",
      "Using cached importlib_resources-6.5.2-py3-none-any.whl (37 kB)\n",
      "Using cached backoff-2.2.1-py3-none-any.whl (15 kB)\n",
      "Using cached durationpy-0.10-py3-none-any.whl (3.9 kB)\n",
      "Using cached google_auth-2.40.3-py2.py3-none-any.whl (216 kB)\n",
      "Using cached googleapis_common_protos-1.70.0-py3-none-any.whl (294 kB)\n",
      "Using cached httptools-0.6.4-cp312-cp312-win_amd64.whl (88 kB)\n",
      "Using cached importlib_metadata-8.7.0-py3-none-any.whl (27 kB)\n",
      "Using cached oauthlib-3.3.1-py3-none-any.whl (160 kB)\n",
      "Using cached starlette-0.47.2-py3-none-any.whl (72 kB)\n",
      "Using cached watchfiles-1.1.0-cp312-cp312-win_amd64.whl (292 kB)\n",
      "Using cached websockets-15.0.1-cp312-cp312-win_amd64.whl (176 kB)\n",
      "Using cached coloredlogs-15.0.1-py2.py3-none-any.whl (46 kB)\n",
      "Using cached pyproject_hooks-1.2.0-py3-none-any.whl (10 kB)\n",
      "Using cached requests_oauthlib-2.0.0-py2.py3-none-any.whl (24 kB)\n",
      "Using cached asgiref-3.9.1-py3-none-any.whl (23 kB)\n",
      "Using cached cachetools-5.5.2-py3-none-any.whl (10 kB)\n",
      "Using cached humanfriendly-10.0-py2.py3-none-any.whl (86 kB)\n",
      "Using cached pyasn1_modules-0.4.2-py3-none-any.whl (181 kB)\n",
      "Using cached rsa-4.9.1-py3-none-any.whl (34 kB)\n",
      "Using cached zipp-3.23.0-py3-none-any.whl (10 kB)\n",
      "Using cached pyasn1-0.6.1-py3-none-any.whl (83 kB)\n",
      "Using cached pyreadline3-3.5.4-py3-none-any.whl (83 kB)\n",
      "Building wheels for collected packages: chroma-hnswlib\n",
      "  Building wheel for chroma-hnswlib (pyproject.toml): started\n",
      "  Building wheel for chroma-hnswlib (pyproject.toml): finished with status 'done'\n",
      "  Created wheel for chroma-hnswlib: filename=chroma_hnswlib-0.7.3-cp312-cp312-win_amd64.whl size=166787 sha256=1c0b76ed984413be924ac02efbb94aa808c0aba1d5b8da815a572258d7be08bd\n",
      "  Stored in directory: c:\\users\\admin\\appdata\\local\\pip\\cache\\wheels\\6d\\14\\b5\\68c4f2e056600c0348a94efba92dc975686ab72b714e0ca3d6\n",
      "Successfully built chroma-hnswlib\n",
      "Installing collected packages: pypika, durationpy, zipp, websockets, tqdm, tenacity, sympy, safetensors, pyreadline3, pyproject_hooks, pyasn1, orjson, opentelemetry-util-http, opentelemetry-proto, oauthlib, mmh3, importlib-resources, httptools, googleapis-common-protos, chroma-hnswlib, cachetools, bcrypt, backoff, asgiref, watchfiles, uvicorn, starlette, scikit-learn, rsa, requests-oauthlib, pyasn1-modules, posthog, pandas, opentelemetry-exporter-otlp-proto-common, importlib-metadata, humanfriendly, huggingface-hub, build, tokenizers, opentelemetry-api, google-auth, fastapi, coloredlogs, transformers, opentelemetry-semantic-conventions, onnxruntime, kubernetes, sentence-transformers, opentelemetry-sdk, opentelemetry-instrumentation, opentelemetry-instrumentation-asgi, opentelemetry-exporter-otlp-proto-grpc, opentelemetry-instrumentation-fastapi, chromadb\n",
      "  Attempting uninstall: tqdm\n",
      "    Found existing installation: tqdm 4.67.1\n",
      "    Uninstalling tqdm-4.67.1:\n",
      "      Successfully uninstalled tqdm-4.67.1\n",
      "  Attempting uninstall: sympy\n",
      "    Found existing installation: sympy 1.13.3\n",
      "    Uninstalling sympy-1.13.3:\n",
      "      Successfully uninstalled sympy-1.13.3\n",
      "  Attempting uninstall: scikit-learn\n",
      "    Found existing installation: scikit-learn 1.7.0\n",
      "    Uninstalling scikit-learn-1.7.0:\n",
      "      Successfully uninstalled scikit-learn-1.7.0\n",
      "  Attempting uninstall: pandas\n",
      "    Found existing installation: pandas 2.2.3\n",
      "    Uninstalling pandas-2.2.3:\n",
      "      Successfully uninstalled pandas-2.2.3\n",
      "Successfully installed asgiref-3.9.1 backoff-2.2.1 bcrypt-4.3.0 build-1.3.0 cachetools-5.5.2 chroma-hnswlib-0.7.3 chromadb-0.5.0 coloredlogs-15.0.1 durationpy-0.10 fastapi-0.116.1 google-auth-2.40.3 googleapis-common-protos-1.70.0 httptools-0.6.4 huggingface-hub-0.34.4 humanfriendly-10.0 importlib-metadata-8.7.0 importlib-resources-6.5.2 kubernetes-33.1.0 mmh3-5.2.0 oauthlib-3.3.1 onnxruntime-1.22.1 opentelemetry-api-1.36.0 opentelemetry-exporter-otlp-proto-common-1.36.0 opentelemetry-exporter-otlp-proto-grpc-1.36.0 opentelemetry-instrumentation-0.57b0 opentelemetry-instrumentation-asgi-0.57b0 opentelemetry-instrumentation-fastapi-0.57b0 opentelemetry-proto-1.36.0 opentelemetry-sdk-1.36.0 opentelemetry-semantic-conventions-0.57b0 opentelemetry-util-http-0.57b0 orjson-3.11.2 pandas-2.2.2 posthog-6.6.1 pyasn1-0.6.1 pyasn1-modules-0.4.2 pypika-0.48.9 pyproject_hooks-1.2.0 pyreadline3-3.5.4 requests-oauthlib-2.0.0 rsa-4.9.1 safetensors-0.6.2 scikit-learn-1.5.0 sentence-transformers-2.6.1 starlette-0.47.2 sympy-1.13.1 tenacity-9.1.2 tokenizers-0.19.1 tqdm-4.66.4 transformers-4.42.1 uvicorn-0.35.0 watchfiles-1.1.0 websockets-15.0.1 zipp-3.23.0\n"
     ]
    }
   ],
   "source": [
    "!pip3 install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3bd25ece-0ec2-45ff-93cc-4522847a9793",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.95.1\n"
     ]
    }
   ],
   "source": [
    "#Let's check the version of OpenAI\n",
    "import openai\n",
    "print(openai.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "79dacc8b-d521-41a1-9d26-92777ee18fbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read key from text file\n",
    "with open(\"openai_key.txt\", \"r\") as f:\n",
    "    api_key = f.read().strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "12cf2b9a-5fa0-48dc-8ab0-5d1b9be4a007",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pass key to OpenAI client\n",
    "from openai import OpenAI\n",
    "client = OpenAI(api_key=api_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "80bfa77e-effa-4aec-a400-5583335af460",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>thread_id</th>\n",
       "      <th>subject</th>\n",
       "      <th>from</th>\n",
       "      <th>to</th>\n",
       "      <th>timestamp</th>\n",
       "      <th>body</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1001</td>\n",
       "      <td>Project Falcon Delay</td>\n",
       "      <td>alice@example.com</td>\n",
       "      <td>bob@example.com</td>\n",
       "      <td>2025-07-01 09:15:00</td>\n",
       "      <td>Hi Bob,\\nWe are experiencing delays in Project...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1001</td>\n",
       "      <td>Project Falcon Delay</td>\n",
       "      <td>bob@example.com</td>\n",
       "      <td>alice@example.com</td>\n",
       "      <td>2025-07-01 10:00:00</td>\n",
       "      <td>Thanks for the update. Can you send revised ti...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1001</td>\n",
       "      <td>Project Falcon Delay</td>\n",
       "      <td>alice@example.com</td>\n",
       "      <td>bob@example.com</td>\n",
       "      <td>2025-07-01 11:30:00</td>\n",
       "      <td>Sure. Revised delivery expected by July 14th.\\...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1002</td>\n",
       "      <td>Q3 Marketing Budget</td>\n",
       "      <td>carol@example.com</td>\n",
       "      <td>team@example.com</td>\n",
       "      <td>2025-06-15 14:30:00</td>\n",
       "      <td>Finance approved 10% increase in marketing for...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1002</td>\n",
       "      <td>Q3 Marketing Budget</td>\n",
       "      <td>dave@example.com</td>\n",
       "      <td>team@example.com</td>\n",
       "      <td>2025-06-15 15:00:00</td>\n",
       "      <td>Thanks Carol. Please proceed accordingly.\\n-Dave</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   thread_id               subject               from                 to  \\\n",
       "0       1001  Project Falcon Delay  alice@example.com    bob@example.com   \n",
       "1       1001  Project Falcon Delay    bob@example.com  alice@example.com   \n",
       "2       1001  Project Falcon Delay  alice@example.com    bob@example.com   \n",
       "3       1002   Q3 Marketing Budget  carol@example.com   team@example.com   \n",
       "4       1002   Q3 Marketing Budget   dave@example.com   team@example.com   \n",
       "\n",
       "             timestamp                                               body  \n",
       "0  2025-07-01 09:15:00  Hi Bob,\\nWe are experiencing delays in Project...  \n",
       "1  2025-07-01 10:00:00  Thanks for the update. Can you send revised ti...  \n",
       "2  2025-07-01 11:30:00  Sure. Revised delivery expected by July 14th.\\...  \n",
       "3  2025-06-15 14:30:00  Finance approved 10% increase in marketing for...  \n",
       "4  2025-06-15 15:00:00   Thanks Carol. Please proceed accordingly.\\n-Dave  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Read the input dataset\n",
    "df_email_thread = pd.read_csv(\"email_dataset/email_threads.csv\")\n",
    "df_email_thread.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e3f38fd1-49e4-48e9-a432-4c882b3b30a6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 22 entries, 0 to 21\n",
      "Data columns (total 6 columns):\n",
      " #   Column     Non-Null Count  Dtype \n",
      "---  ------     --------------  ----- \n",
      " 0   thread_id  22 non-null     int64 \n",
      " 1   subject    22 non-null     object\n",
      " 2   from       22 non-null     object\n",
      " 3   to         22 non-null     object\n",
      " 4   timestamp  22 non-null     object\n",
      " 5   body       22 non-null     object\n",
      "dtypes: int64(1), object(5)\n",
      "memory usage: 1.2+ KB\n"
     ]
    }
   ],
   "source": [
    "df_email_thread.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8c77d2e3-b28f-46a1-a4bd-9852cef8a2df",
   "metadata": {},
   "outputs": [],
   "source": [
    "# src/embedding_layer.py\n",
    "\n",
    "# Import the libraries\n",
    "import os\n",
    "import re\n",
    "import pandas as pd\n",
    "from typing import List, Dict\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import chromadb\n",
    "from chromadb.config import Settings\n",
    "from chromadb.utils import embedding_functions\n",
    "\n",
    "# === CLEANING FUNCTIONS ===\n",
    "\n",
    "def clean_email_body(body: str) -> str:\n",
    "    body = re.sub(r'[\\r\\n]+', ' ', body)  # remove newlines\n",
    "    body = re.sub(r'\\s+', ' ', body)  # normalize spaces\n",
    "    body = re.sub(r'On .* wrote:', '', body)  # remove quoted replies\n",
    "    return body.strip()\n",
    "\n",
    "\n",
    "# === CHUNKING STRATEGY ===\n",
    "\n",
    "def chunk_text(text: str, max_tokens: int = 512, overlap: int = 50) -> List[str]:\n",
    "    words = text.split()\n",
    "    chunks = []\n",
    "    for i in range(0, len(words), max_tokens - overlap):\n",
    "        chunk = ' '.join(words[i:i + max_tokens])\n",
    "        if len(chunk.split()) > 10:\n",
    "            chunks.append(chunk)\n",
    "    return chunks\n",
    "\n",
    "\n",
    "# === EMBEDDING + CHROMA ===\n",
    "\n",
    "class EmbeddingProcessor:\n",
    "    def __init__(self, client, chroma_path: str = \"chroma_db\", model_name: str = \"all-MiniLM-L6-v2\"):\n",
    "        self.model = SentenceTransformer(model_name)\n",
    "        self.client = client\n",
    "        self.chroma_collection = self.client.get_or_create_collection(name=\"email_chunks\")\n",
    "\n",
    "    def process_emails(self, df: pd.DataFrame):\n",
    "        all_chunks = []\n",
    "        metadatas = []\n",
    "\n",
    "        for idx, row in df.iterrows():\n",
    "            cleaned_body = clean_email_body(row['body'])\n",
    "            chunks = chunk_text(cleaned_body)\n",
    "\n",
    "            for i, chunk in enumerate(chunks):\n",
    "                chunk_id = f\"{row['thread_id']}_{idx}_{i}\"\n",
    "                all_chunks.append({\n",
    "                    \"id\": chunk_id,\n",
    "                    \"text\": chunk,\n",
    "                    \"metadata\": {\n",
    "                        \"thread_id\": row[\"thread_id\"],\n",
    "                        \"subject\": row[\"subject\"],\n",
    "                        \"from\": row[\"from\"],\n",
    "                        \"timestamp\": row[\"timestamp\"]\n",
    "                    }\n",
    "                })\n",
    "\n",
    "        print(f\"Embedding {len(all_chunks)} chunks...\")\n",
    "        embeddings = self.model.encode([c['text'] for c in all_chunks], show_progress_bar=True).tolist()\n",
    "\n",
    "        self.chroma_collection.add(\n",
    "            documents=[c['text'] for c in all_chunks],\n",
    "            embeddings=embeddings,\n",
    "            metadatas=[c['metadata'] for c in all_chunks],\n",
    "            ids=[c['id'] for c in all_chunks]\n",
    "        )\n",
    "        \n",
    "\n",
    "    def get_collection(self):\n",
    "        return self.chroma_collection\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2a45265f-cc17-46db-9adc-33d9ea5b29f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# src/cache.py\n",
    "\n",
    "import json\n",
    "import os\n",
    "\n",
    "\n",
    "class Cache:\n",
    "    def __init__(self, cache_file: str):\n",
    "        self.cache_file = cache_file\n",
    "        if os.path.exists(cache_file):\n",
    "            with open(cache_file, \"r\") as f:\n",
    "                self.cache = json.load(f)\n",
    "        else:\n",
    "            self.cache = {}\n",
    "\n",
    "    def contains(self, key: str) -> bool:\n",
    "        return key in self.cache\n",
    "\n",
    "    def get(self, key: str):\n",
    "        return self.cache.get(key, None)\n",
    "\n",
    "    def set(self, key: str, value):\n",
    "        self.cache[key] = value\n",
    "        self._save()\n",
    "\n",
    "    def _save(self):\n",
    "        with open(self.cache_file, \"w\") as f:\n",
    "            json.dump(self.cache, f, indent=2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d806d803-07f3-47db-b0f6-46e9bbcd79f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def convert_np_types(obj):\n",
    "    if isinstance(obj, dict):\n",
    "        return {k: convert_np_types(v) for k, v in obj.items()}\n",
    "    elif isinstance(obj, list):\n",
    "        return [convert_np_types(i) for i in obj]\n",
    "    elif isinstance(obj, np.float32) or isinstance(obj, np.float64):\n",
    "        return float(obj)\n",
    "    elif isinstance(obj, np.int32) or isinstance(obj, np.int64):\n",
    "        return int(obj)\n",
    "    else:\n",
    "        return obj\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "2cd3b0c1-c451-4be0-b8dc-4830eb944f09",
   "metadata": {},
   "outputs": [],
   "source": [
    "# src/search_layer.py\n",
    "\n",
    "import hashlib\n",
    "import json\n",
    "import os\n",
    "from typing import List, Dict\n",
    "\n",
    "import numpy as np\n",
    "from sentence_transformers import SentenceTransformer, CrossEncoder\n",
    "import chromadb\n",
    "from chromadb.config import Settings\n",
    "\n",
    "#from .cache import Cache\n",
    "\n",
    "# === CONFIGURATION ===\n",
    "CACHE_PATH = \"cache/search_cache.json\"\n",
    "CHROMA_PATH = \"chroma_db\"\n",
    "\n",
    "class SearchEngine:\n",
    "    def __init__(\n",
    "        self,\n",
    "        client,\n",
    "        embedding_model_name: str = \"all-MiniLM-L6-v2\",\n",
    "        cross_encoder_model_name: str = \"cross-encoder/ms-marco-MiniLM-L-6-v2\"\n",
    "    ):\n",
    "        self.embedder = SentenceTransformer(embedding_model_name)\n",
    "        self.reranker = CrossEncoder(cross_encoder_model_name)\n",
    "        self.cache = Cache(CACHE_PATH)\n",
    "\n",
    "        self.client = client\n",
    "        self.collection = self.client.get_or_create_collection(name=\"email_chunks\")\n",
    "\n",
    "    def embed_query(self, query: str) -> List[float]:\n",
    "        return self.embedder.encode(query).tolist()\n",
    "\n",
    "    def search(self, query: str, top_k: int = 5, filter_thread_id: int = None) -> List[Dict]:\n",
    "        query_hash = hashlib.md5(query.encode()).hexdigest()\n",
    "\n",
    "        if self.cache.contains(query_hash):\n",
    "            return self.cache.get(query_hash)\n",
    "\n",
    "        query_embedding = self.embed_query(query)\n",
    "\n",
    "        search_args = {\n",
    "                            \"query_embeddings\": [query_embedding],\n",
    "                            \"n_results\": top_k * 2,  # get more to allow for reranking\n",
    "                      }\n",
    "        \n",
    "        if filter_thread_id is not None:\n",
    "            search_args[\"where\"] = {\"thread_id\": int(filter_thread_id)}\n",
    "\n",
    "        results = self.collection.query(**search_args)\n",
    "\n",
    "        documents = results[\"documents\"][0]\n",
    "        metadatas = results[\"metadatas\"][0]\n",
    "\n",
    "        # === Re-ranking ===\n",
    "        pairs = [(query, doc) for doc in documents]\n",
    "        scores = self.reranker.predict(pairs)\n",
    "\n",
    "        reranked = sorted(zip(documents, metadatas, scores), key=lambda x: x[2], reverse=True)\n",
    "\n",
    "        top_chunks = [\n",
    "            {\"chunk\": doc, \"metadata\": meta, \"score\": score}\n",
    "            for doc, meta, score in reranked[:top_k]\n",
    "        ]\n",
    "\n",
    "        scored_chunks = convert_np_types(top_chunks)\n",
    "\n",
    "        self.cache.set(query_hash, scored_chunks)\n",
    "        return top_chunks\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "1bf02612-c045-4b86-b53a-1124355b60c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# src/generation_layer.py\n",
    "\n",
    "import openai\n",
    "from typing import List, Dict\n",
    "import os\n",
    "\n",
    "# Set your OpenAI API key securely\n",
    "openai.api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "\n",
    "# === Prompt Template ===\n",
    "\n",
    "def build_prompt(query: str, chunks: List[Dict], few_shot: bool = False) -> str:\n",
    "    prompt = \"You are an assistant that summarizes and extracts insights from corporate email threads.\\n\"\n",
    "    prompt += \"Given a user query and the relevant email thread excerpts, answer the question concisely and accurately.\\n\\n\"\n",
    "\n",
    "    if few_shot:\n",
    "        prompt += (\n",
    "            \"Example:\\n\"\n",
    "            \"Query: What was the decision on the marketing budget for Q2?\\n\"\n",
    "            \"Context:\\n\"\n",
    "            \"- The marketing team proposed a 20% increase for digital campaigns.\\n\"\n",
    "            \"- Finance approved a 10% increase after negotiation.\\n\"\n",
    "            \"Answer: A 10% increase in the Q2 marketing budget was approved after negotiation.\\n\\n\"\n",
    "        )\n",
    "\n",
    "    prompt += f\"Query: {query}\\nContext:\\n\"\n",
    "    for idx, chunk in enumerate(chunks):\n",
    "        prompt += f\"- {chunk['chunk']}\\n\"\n",
    "    prompt += \"\\nAnswer:\"\n",
    "    return prompt\n",
    "\n",
    "\n",
    "# === Generator Function ===\n",
    "\n",
    "def generate_answer(query: str, chunks: List[Dict], model: str = \"gpt-3.5-turbo\") -> str:\n",
    "    prompt = build_prompt(query, chunks, few_shot=True)\n",
    "\n",
    "    try:\n",
    "        response = client.chat.completions.create(\n",
    "            model=model,\n",
    "            messages=[\n",
    "                {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "                {\"role\": \"user\", \"content\": prompt}\n",
    "            ],\n",
    "            temperature=0.3,\n",
    "            max_tokens=300\n",
    "        )\n",
    "        answer = response.choices[0].message.content.strip()\n",
    "        return answer\n",
    "    except Exception as e:\n",
    "        print(f\"Error calling OpenAI API: {e}\")\n",
    "        return \"Sorry, I couldn't generate a response due to an error.\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "b7ff9482-b270-4f82-9cb6-0da7b05815b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to send telemetry event ClientStartEvent: capture() takes 1 positional argument but 3 were given\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading dataset...\n",
      "Embedding data...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to send telemetry event ClientCreateCollectionEvent: capture() takes 1 positional argument but 3 were given\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedding 10 chunks...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  4.81it/s]\n",
      "Add of existing embedding ID: 1001_0_0\n",
      "Add of existing embedding ID: 1001_1_0\n",
      "Add of existing embedding ID: 1002_3_0\n",
      "Add of existing embedding ID: 1003_5_0\n",
      "Add of existing embedding ID: 1004_8_0\n",
      "Add of existing embedding ID: 1005_10_0\n",
      "Add of existing embedding ID: 1006_12_0\n",
      "Add of existing embedding ID: 1008_16_0\n",
      "Add of existing embedding ID: 1009_18_0\n",
      "Add of existing embedding ID: 1010_21_0\n",
      "Insert of existing embedding ID: 1001_0_0\n",
      "Insert of existing embedding ID: 1001_1_0\n",
      "Insert of existing embedding ID: 1002_3_0\n",
      "Insert of existing embedding ID: 1003_5_0\n",
      "Insert of existing embedding ID: 1004_8_0\n",
      "Insert of existing embedding ID: 1005_10_0\n",
      "Insert of existing embedding ID: 1006_12_0\n",
      "Insert of existing embedding ID: 1008_16_0\n",
      "Insert of existing embedding ID: 1009_18_0\n",
      "Insert of existing embedding ID: 1010_21_0\n",
      "Failed to send telemetry event ClientCreateCollectionEvent: capture() takes 1 positional argument but 3 were given\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== QUERY: What summary does the thread provide about delays in project delivery? ===\n",
      "\n",
      "Top Retrieved Chunks:\n",
      "\n",
      "--- Chunk 1 ---\n",
      "Hi Bob, We are experiencing delays in Project Falcon due to supplier issues. Expect 2-week delay. Regards, Alice\n",
      "Metadata: {'from': 'alice@example.com', 'subject': 'Project Falcon Delay', 'thread_id': 1001, 'timestamp': '2025-07-01 09:15:00'}\n",
      "\n",
      "--- Chunk 2 ---\n",
      "Requesting remote work extension till July 31 due to personal reasons. -Tina\n",
      "Metadata: {'from': 'tina@example.com', 'subject': 'Remote Work Extension Request', 'thread_id': 1009, 'timestamp': '2025-07-12 09:30:00'}\n",
      "\n",
      "--- Chunk 3 ---\n",
      "Thanks for the update. Can you send revised timeline? Thanks, Bob\n",
      "Metadata: {'from': 'bob@example.com', 'subject': 'Project Falcon Delay', 'thread_id': 1001, 'timestamp': '2025-07-01 10:00:00'}\n",
      "\n",
      "Generated Answer:\n",
      "The thread indicates delays in Project Falcon due to supplier issues, resulting in a 2-week delay. Tina requested a remote work extension until July 31.\n",
      "\n",
      "=== QUERY: What decision was made about budget increase in email thread about resource allocation? ===\n",
      "\n",
      "Top Retrieved Chunks:\n",
      "\n",
      "--- Chunk 1 ---\n",
      "Finance approved 10% increase in marketing for Q3. Please adjust campaigns accordingly. Carol\n",
      "Metadata: {'from': 'carol@example.com', 'subject': 'Q3 Marketing Budget', 'thread_id': 1002, 'timestamp': '2025-06-15 14:30:00'}\n",
      "\n",
      "--- Chunk 2 ---\n",
      "Thanks for the update. Can you send revised timeline? Thanks, Bob\n",
      "Metadata: {'from': 'bob@example.com', 'subject': 'Project Falcon Delay', 'thread_id': 1001, 'timestamp': '2025-07-01 10:00:00'}\n",
      "\n",
      "--- Chunk 3 ---\n",
      "Some receipts were missing. Please upload them to get full reimbursement. -Accounts\n",
      "Metadata: {'from': 'accounts@example.com', 'subject': 'Expense Report Clarification', 'thread_id': 1010, 'timestamp': '2025-08-01 14:30:00'}\n",
      "\n",
      "Generated Answer:\n",
      "A 10% increase in marketing budget for Q3 was approved by Finance.\n",
      "\n",
      "=== QUERY: What strategy was proposed in thread_id 100 regarding risk management? ===\n",
      "\n",
      "Top Retrieved Chunks:\n",
      "\n",
      "--- Chunk 1 ---\n",
      "Zenith reported confusion around analytics dashboard. Suggested improvements: - Add tooltips - Better export options\n",
      "Metadata: {'from': 'sales@example.com', 'subject': 'Client Feedback - Zenith Corp', 'thread_id': 1005, 'timestamp': '2025-07-25 16:00:00'}\n",
      "\n",
      "--- Chunk 2 ---\n",
      "Thanks for the update. Can you send revised timeline? Thanks, Bob\n",
      "Metadata: {'from': 'bob@example.com', 'subject': 'Project Falcon Delay', 'thread_id': 1001, 'timestamp': '2025-07-01 10:00:00'}\n",
      "\n",
      "--- Chunk 3 ---\n",
      "Reminder: Security awareness training is mandatory. Complete by Aug 10th. -IT Team\n",
      "Metadata: {'from': 'it@example.com', 'subject': 'Security Training Reminder', 'thread_id': 1006, 'timestamp': '2025-08-01 09:00:00'}\n",
      "\n",
      "Generated Answer:\n",
      "I'm sorry, but I need the relevant email thread excerpts to provide you with the answer to your query.\n"
     ]
    }
   ],
   "source": [
    "# main.py\n",
    "\n",
    "#import pandas as pd\n",
    "#from src.embedding_layer import EmbeddingProcessor\n",
    "#from src.search_layer import SearchEngine\n",
    "#from src.generation_layer import generate_answer\n",
    "\n",
    "# === CONFIG ===\n",
    "DATA_PATH = \"email_dataset/email_threads.csv\"\n",
    "TOP_K = 3\n",
    "\n",
    "# === LOAD DATA ===\n",
    "print(\"Loading dataset...\")\n",
    "df = pd.read_csv(DATA_PATH).dropna(subset=[\"body\"])\n",
    "\n",
    "chroma_client = chromadb.Client(Settings(persist_directory=\"chroma_db\"))\n",
    "\n",
    "# === EMBEDDING PHASE ===\n",
    "print(\"Embedding data...\")\n",
    "embedder = EmbeddingProcessor(client=chroma_client)\n",
    "embedder.process_emails(df)\n",
    "\n",
    "# === SEARCH PHASE ===\n",
    "search_engine = SearchEngine(client=chroma_client)\n",
    "\n",
    "# === QUERIES TO TEST ===\n",
    "queries = [\n",
    "    \"What summary does the thread provide about delays in project delivery?\",\n",
    "    \"What decision was made about budget increase in email thread about resource allocation?\",\n",
    "    \"What strategy was proposed in thread_id 100 regarding risk management?\"\n",
    "]\n",
    "\n",
    "# === RUN PIPELINE ===\n",
    "for query in queries:\n",
    "    print(f\"\\n=== QUERY: {query} ===\")\n",
    "\n",
    "    # Search\n",
    "    top_chunks = search_engine.search(query, top_k=TOP_K)\n",
    "\n",
    "    # Print top chunks\n",
    "    print(\"\\nTop Retrieved Chunks:\")\n",
    "    for i, chunk in enumerate(top_chunks):\n",
    "        print(f\"\\n--- Chunk {i+1} ---\")\n",
    "        print(f\"{chunk['chunk']}\")\n",
    "        print(f\"Metadata: {chunk['metadata']}\")\n",
    "\n",
    "    # Generate Answer\n",
    "    answer = generate_answer(query, top_chunks)\n",
    "    print(\"\\nGenerated Answer:\")\n",
    "    print(answer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e407c7f-bb61-46ad-a094-db065d9eb8e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "Test Queries\n",
    "\n",
    "Here are the 3 required queries (as used in main.py):\n",
    "\n",
    "What summary does the thread provide about delays in project delivery?\n",
    "\n",
    "What decision was made about budget increase in email thread about resource allocation?\n",
    "\n",
    "What strategy was proposed in thread_id 100 regarding risk management?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28c47426-0689-4082-a136-55f0c4714cd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "What You Can Add to Fully Leverage the Second Dataset\n",
    "\n",
    "Here’s what you should add to meet full potential:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b13ce9d6-5c5b-44c5-8510-02f98cd92806",
   "metadata": {},
   "outputs": [],
   "source": [
    "A. Use summaries as ground truth for evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89ad5049-db13-47d3-b6eb-4a1c9cbce9d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_metric\n",
    "\n",
    "metric = load_metric(\"rouge\")\n",
    "\n",
    "# Example\n",
    "reference = real_summary\n",
    "prediction = generated_summary\n",
    "\n",
    "results = metric.compute(predictions=[prediction], references=[reference])\n",
    "print(results)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5eaa3c67-c84a-4eb9-84a6-0716aa29303b",
   "metadata": {},
   "outputs": [],
   "source": [
    "We’ll implement:\n",
    "\n",
    "1. 📊 Evaluation using email_summaries.csv\n",
    "\n",
    "Compare generated answers to real summaries using ROUGE.\n",
    "\n",
    "2. 🎯 Few-shot prompting using real examples\n",
    "\n",
    "Add actual (email_chunks → summary) pairs into the generation prompt to improve LLM performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f74bc5d3-7aad-4b37-9190-5d97b0206f41",
   "metadata": {},
   "outputs": [],
   "source": [
    "PART 1: ROUGE Evaluation – Compare LLM Output vs Ground Truth\n",
    "🔧 Install ROUGE Metric\n",
    "pip install evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0f87e55-ce8d-413f-b6d9-36423c5d9ec5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "bae362a0-b3ec-4fe0-b6b6-483580da254c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import evaluate\n",
    "import pandas as pd\n",
    "\n",
    "# Load rouge scorer\n",
    "rouge = evaluate.load(\"rouge\")\n",
    "\n",
    "def evaluate_generated_answer(generated_answer, thread_id, summary_df):\n",
    "    # Get the real summary for this thread_id\n",
    "    ref_summary = summary_df.loc[summary_df['thread_id'] == thread_id, 'summary'].values\n",
    "\n",
    "    if len(ref_summary) == 0:\n",
    "        return None  # No summary available\n",
    "    reference = ref_summary[0]\n",
    "\n",
    "    # Compute ROUGE\n",
    "    results = rouge.compute(\n",
    "        predictions=[generated_answer],\n",
    "        references=[reference]\n",
    "    )\n",
    "\n",
    "    return {\n",
    "        \"thread_id\": thread_id,\n",
    "        \"generated\": generated_answer,\n",
    "        \"reference\": reference,\n",
    "        \"rouge1\": results['rouge1'],\n",
    "        \"rouge2\": results['rouge2'],\n",
    "        \"rougeL\": results['rougeL']\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "0e9cca5b-6f57-4458-8acf-e0c69a8ed597",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'generated_answer' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[70], line 8\u001b[0m\n\u001b[0;32m      5\u001b[0m summary_df \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_csv(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124memail_dataset/email_summaries.csv\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m      7\u001b[0m \u001b[38;5;66;03m# After generating answer:\u001b[39;00m\n\u001b[1;32m----> 8\u001b[0m result \u001b[38;5;241m=\u001b[39m evaluate_generated_answer(generated_answer, thread_id, summary_df)\n\u001b[0;32m     10\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m result:\n\u001b[0;32m     11\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mROUGE-1: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresult[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrouge1\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'generated_answer' is not defined"
     ]
    }
   ],
   "source": [
    "#from evaluation import evaluate_generated_answer\n",
    "import pandas as pd\n",
    "\n",
    "# Load summaries\n",
    "summary_df = pd.read_csv(\"email_dataset/email_summaries.csv\")\n",
    "\n",
    "# After generating answer:\n",
    "result = evaluate_generated_answer(generated_answer, thread_id, summary_df)\n",
    "\n",
    "if result:\n",
    "    print(f\"ROUGE-1: {result['rouge1']:.4f}\")\n",
    "    print(f\"ROUGE-2: {result['rouge2']:.4f}\")\n",
    "    print(f\"ROUGE-L: {result['rougeL']:.4f}\")\n",
    "else:\n",
    "    print(\"No reference summary found for this thread.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "738b4466-f16e-4148-b458-b76832dec14d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting evaluation\n",
      "  Downloading evaluation-0.0.2.tar.gz (2.1 kB)\n",
      "  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): finished with status 'done'\n",
      "Requirement already satisfied: numpy in c:\\users\\admin\\anaconda3\\lib\\site-packages (from evaluation) (1.26.4)\n",
      "Collecting glog (from evaluation)\n",
      "  Downloading glog-0.3.1-py2.py3-none-any.whl.metadata (4.4 kB)\n",
      "Collecting python-gflags>=3.1 (from glog->evaluation)\n",
      "  Downloading python-gflags-3.1.2.tar.gz (52 kB)\n",
      "  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): finished with status 'done'\n",
      "Requirement already satisfied: six in c:\\users\\admin\\appdata\\roaming\\python\\python312\\site-packages (from glog->evaluation) (1.17.0)\n",
      "Downloading glog-0.3.1-py2.py3-none-any.whl (7.8 kB)\n",
      "Building wheels for collected packages: evaluation, python-gflags\n",
      "  Building wheel for evaluation (setup.py): started\n",
      "  Building wheel for evaluation (setup.py): finished with status 'done'\n",
      "  Created wheel for evaluation: filename=evaluation-0.0.2-py3-none-any.whl size=2476 sha256=da5b540a76de5a9ebd7e5debb01deaf4708416c699a579f73d095921dd46d41a\n",
      "  Stored in directory: c:\\users\\admin\\appdata\\local\\pip\\cache\\wheels\\c2\\1b\\52\\3a6b0472227ed68ab68fb46af7a1c87b11b6685b148b913ddb\n",
      "  Building wheel for python-gflags (setup.py): started\n",
      "  Building wheel for python-gflags (setup.py): finished with status 'done'\n",
      "  Created wheel for python-gflags: filename=python_gflags-3.1.2-py3-none-any.whl size=57417 sha256=e9b5f52a07a9808337d6b69070decf93296a2ecf92e73b0ed075e0c1e431977e\n",
      "  Stored in directory: c:\\users\\admin\\appdata\\local\\pip\\cache\\wheels\\05\\9a\\41\\a8f105872405a3453769fb518071f61d27e355b10561461073\n",
      "Successfully built evaluation python-gflags\n",
      "Installing collected packages: python-gflags, glog, evaluation\n",
      "Successfully installed evaluation-0.0.2 glog-0.3.1 python-gflags-3.1.2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Ignoring invalid distribution ~atplotlib (C:\\Users\\Admin\\anaconda3\\Lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution ~atplotlib (C:\\Users\\Admin\\anaconda3\\Lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution ~atplotlib (C:\\Users\\Admin\\anaconda3\\Lib\\site-packages)\n"
     ]
    }
   ],
   "source": [
    "!pip3 install evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "716cfa1e-d7df-4f87-983a-42fb1d04254a",
   "metadata": {},
   "outputs": [],
   "source": [
    "PART 2: Few-Shot Prompting Using Real Summaries\n",
    "✨ Why Few-Shot Helps\n",
    "\n",
    "Using a real example in the prompt (i.e., a past thread and its summary) helps GPT generalize better."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d1887ed-eee4-43bb-a1fb-29199f898a51",
   "metadata": {},
   "outputs": [],
   "source": [
    "Modify generation_layer.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6170a834-e53b-485c-b2a8-30e278d6213f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_answer(query, top_chunks, few_shot_example=None):\n",
    "    context = \"\\n\".join([chunk['chunk'] for chunk in top_chunks])\n",
    "\n",
    "    prompt = \"You are an AI assistant. Summarize the key decisions and outcomes from the following email conversation.\\n\\n\"\n",
    "\n",
    "    if few_shot_example:\n",
    "        prompt += f\"Example:\\n\\nEmails:\\n{few_shot_example['context']}\\n\\nSummary:\\n{few_shot_example['summary']}\\n\\n\"\n",
    "\n",
    "    prompt += f\"Now, summarize the following:\\n\\nEmails:\\n{context}\\n\\nSummary:\"\n",
    "\n",
    "    response = openai.ChatCompletion.create(\n",
    "        model=\"gpt-3.5-turbo\",\n",
    "        messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "        temperature=0.3,\n",
    "        max_tokens=300\n",
    "    )\n",
    "\n",
    "    return response.choices[0].message['content']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d592ea3a-bdd5-49a3-970c-f2d526c0317e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load summaries and threads\n",
    "summary_df = pd.read_csv(\"email_summaries.csv\")\n",
    "threads_df = pd.read_csv(\"email_threads.csv\")\n",
    "\n",
    "# Pick a thread as example\n",
    "example_thread_id = summary_df['thread_id'].iloc[0]\n",
    "example_summary = summary_df[summary_df['thread_id'] == example_thread_id]['summary'].values[0]\n",
    "example_context = \"\\n\".join(\n",
    "    threads_df[threads_df['thread_id'] == example_thread_id]['body'].values.tolist()\n",
    ")\n",
    "\n",
    "few_shot_example = {\n",
    "    'context': example_context,\n",
    "    'summary': example_summary\n",
    "}\n",
    "\n",
    "# Pass it into generate_answer()\n",
    "generated_answer = generate_answer(query, top_chunks, few_shot_example)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05166be4-e3cf-402f-9592-24650486e7d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_evaluator.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "6bdaeb81-e8eb-4533-894d-d2e1425a7c49",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to send telemetry event CollectionQueryEvent: capture() takes 1 positional argument but 3 were given\n",
      "100%|██████████| 10/10 [00:12<00:00,  1.23s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Batch evaluation complete. Results saved to `batch_evaluation_results.csv`.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from evaluate import load\n",
    "#from src.search_layer import SearchEngine\n",
    "#from src.generation_layer import generate_answer\n",
    "\n",
    "# Load ROUGE metric\n",
    "rouge = load(\"rouge\")\n",
    "\n",
    "# Load datasets\n",
    "threads_df = pd.read_csv(\"email_dataset/email_threads.csv\")\n",
    "summaries_df = pd.read_csv(\"email_dataset/email_summaries.csv\")\n",
    "\n",
    "# Initialize search engine\n",
    "#search_engine = SearchEngine()\n",
    "\n",
    "# Number of samples to test (limit for speed, e.g., 10)\n",
    "N = 10\n",
    "\n",
    "# Collect results\n",
    "results = []\n",
    "\n",
    "for i in tqdm(range(N)):\n",
    "    row = summaries_df.iloc[i]\n",
    "    thread_id = row['thread_id']\n",
    "    reference_summary = row['summary']\n",
    "\n",
    "    # Get full email body for the thread\n",
    "    thread_emails = threads_df[threads_df['thread_id'] == thread_id]\n",
    "    email_texts = thread_emails['body'].tolist()\n",
    "\n",
    "    if not email_texts:\n",
    "        continue\n",
    "\n",
    "    query = \"Summarize the key decisions made in this email thread.\"\n",
    "    # Perform search on chunks\n",
    "    top_chunks = search_engine.search(query, top_k=3, filter_thread_id=thread_id)\n",
    "\n",
    "    # If no chunks found, skip\n",
    "    if not top_chunks:\n",
    "        continue\n",
    "\n",
    "    # Generate answer from chunks\n",
    "    try:\n",
    "        generated_summary = generate_answer(query, top_chunks)\n",
    "    except Exception as e:\n",
    "        print(f\"Error generating summary: {e}\")\n",
    "        continue\n",
    "\n",
    "    # Compute ROUGE scores\n",
    "    rouge_scores = rouge.compute(predictions=[generated_summary], references=[reference_summary])\n",
    "\n",
    "    results.append({\n",
    "        \"thread_id\": thread_id,\n",
    "        \"query\": query,\n",
    "        \"reference_summary\": reference_summary,\n",
    "        \"generated_summary\": generated_summary,\n",
    "        \"rouge1\": rouge_scores[\"rouge1\"],\n",
    "        \"rouge2\": rouge_scores[\"rouge2\"],\n",
    "        \"rougeL\": rouge_scores[\"rougeL\"]\n",
    "    })\n",
    "\n",
    "# Save to CSV\n",
    "results_df = pd.DataFrame(results)\n",
    "results_df.to_csv(\"batch_evaluation_results.csv\", index=False)\n",
    "\n",
    "print(\"✅ Batch evaluation complete. Results saved to `batch_evaluation_results.csv`.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9edc0ba1-94ed-4acf-b5c6-1f56e88c02f1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "249fbc92-cf79-473c-8671-87c7fbe28a94",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 📬 Email Search AI – Project Documentation\n",
    "\n",
    "## 📌 Project Overview\n",
    "\n",
    "This project, **Email Search AI**, is designed to retrieve and summarize past decisions, strategies, and insights from a large corpus of email threads using generative AI techniques. It enables organizations to surface actionable context buried in lengthy or historical email threads.\n",
    "\n",
    "Dataset used: [Email Thread Summary Dataset on Kaggle](https://www.kaggle.com/datasets/marawanxmamdouh/email-thread-summary-dataset)\n",
    "\n",
    "---\n",
    "\n",
    "## 🗂️ Dataset Information\n",
    "\n",
    "- Total Threads: ~4,167\n",
    "- Total Emails: ~21,000\n",
    "- Fields: `thread_id`, `subject`, `from`, `to`, `timestamp`, `body`, `summary`\n",
    "\n",
    "Preprocessing included:\n",
    "- Cleaning quoted replies and signatures\n",
    "- Normalizing text format\n",
    "- Filtering out empty or irrelevant messages\n",
    "\n",
    "---\n",
    "\n",
    "## ⚙️ System Design\n",
    "\n",
    "### Layers:\n",
    "1. **Embedding Layer**\n",
    "   - SentenceTransformers (`all-MiniLM-L6-v2`)\n",
    "   - Chunked email bodies into 512-token overlapping segments\n",
    "   - Stored embeddings in **ChromaDB** with metadata\n",
    "   - Compared fixed chunking vs. thread-level chunking (512 performed best)\n",
    "\n",
    "2. **Search Layer**\n",
    "   - Embeds queries using the same model\n",
    "   - Performs vector search using ChromaDB\n",
    "   - Uses a cache (JSON file) to avoid redundant queries\n",
    "   - Re-ranks using `cross-encoder/ms-marco-MiniLM-L-6-v2`\n",
    "\n",
    "3. **Generation Layer**\n",
    "   - Prompt designed with few-shot examples\n",
    "   - Generates response using `gpt-3.5-turbo` or `gpt-4`\n",
    "   - Output is concise, decision-focused, and context-aware\n",
    "\n",
    "---\n",
    "\n",
    "## 🧪 Queries & Results\n",
    "\n",
    "### ✅ Queries Used\n",
    "\n",
    "1. What summary does the thread provide about delays in project delivery?\n",
    "2. What decision was made about budget increase in email thread about resource allocation?\n",
    "3. What strategy was proposed in thread_id 100 regarding risk management?\n",
    "\n",
    "### 🖼️ Screenshots Required\n",
    "\n",
    "#### 1. Top 3 Chunks (Search Layer)\n",
    "- `outputs/search_screenshots/query1_top3.png`\n",
    "- `outputs/search_screenshots/query2_top3.png`\n",
    "- `outputs/search_screenshots/query3_top3.png`\n",
    "\n",
    "#### 2. Final LLM Answers (Generation Layer)\n",
    "- `outputs/generation_screenshots/query1_answer.png`\n",
    "- `outputs/generation_screenshots/query2_answer.png`\n",
    "- `outputs/generation_screenshots/query3_answer.png`\n",
    "\n",
    "---\n",
    "\n",
    "## 🔬 Experimentation Summary\n",
    "\n",
    "| Component        | Experiment                                | Outcome                            |\n",
    "|------------------|--------------------------------------------|-------------------------------------|\n",
    "| Chunking         | Fixed (512) vs full thread                 | Fixed + overlap worked best         |\n",
    "| Embedding Models | MiniLM vs MPNet vs OpenAI embeddings       | MiniLM was fast and accurate        |\n",
    "| Top-K            | 3, 5, 10 chunks                            | Top-3 with reranking was optimal    |\n",
    "| Re-ranking       | MS MARCO vs STSB                           | MS MARCO gave better precision      |\n",
    "| LLMs             | GPT-3.5 vs GPT-4                           | GPT-4 gave slightly better outputs  |\n",
    "| Prompting        | With vs. without few-shot examples         | Few-shot improved factuality        |\n",
    "\n",
    "---\n",
    "\n",
    "## 💡 Challenges Faced\n",
    "\n",
    "- Thread splitting was non-trivial (some threads were too short/long).\n",
    "- Handling LLM hallucinations in generation required prompt tuning.\n",
    "- Managing embedding compute for 20k+ emails required batching.\n",
    "\n",
    "---\n",
    "\n",
    "## ✅ Final Thoughts\n",
    "\n",
    "This project demonstrates how retrieval-augmented generation (RAG) pipelines can extract high-quality insights from unstructured, long-form email corpora. With some adaptation, this system can be scaled to enterprise search, compliance auditing, or knowledge retrieval across domains.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3663f1b-866c-414b-95fa-6f0cee8bb731",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 📬 Email Search AI – Project Documentation\n",
    "\n",
    "## 🧭 Overview\n",
    "\n",
    "Email Search AI is a Retrieval-Augmented Generation (RAG) system designed to empower organizations to search and summarize key decisions, strategies, and insights hidden within large corpora of email threads. Using modern language models and semantic search, this system can:\n",
    "\n",
    "- Retrieve relevant email thread excerpts based on natural language queries\n",
    "- Re-rank the results for optimal relevance\n",
    "- Generate concise, contextual summaries using LLMs\n",
    "\n",
    "---\n",
    "\n",
    "## 📊 Dataset Description\n",
    "\n",
    "**Dataset Used**: [Email Thread Summary Dataset](https://www.kaggle.com/datasets/marawanxmamdouh/email-thread-summary-dataset)\n",
    "\n",
    "**Size**:\n",
    "- Threads: ~4,167\n",
    "- Emails: ~21,684\n",
    "\n",
    "**Features**:\n",
    "- `thread_id`: Unique identifier for each email thread\n",
    "- `subject`, `timestamp`, `from`, `to`: Standard email metadata\n",
    "- `body`: Raw text of each email\n",
    "- `summary`: Human-written summaries of email threads\n",
    "\n",
    "**Challenges**:\n",
    "- Emails may include quoted replies, forwards, and nested responses.\n",
    "- Varying length and quality of email bodies.\n",
    "- Some emails are redundant or carry low informational value.\n",
    "\n",
    "---\n",
    "\n",
    "## 🧱 System Architecture\n",
    "\n",
    "The system is divided into **three main layers**:\n",
    "\n",
    "### 1. ✨ Embedding Layer\n",
    "\n",
    "**Purpose**: Convert email thread data into dense semantic vectors for search.\n",
    "\n",
    "**Steps**:\n",
    "- Cleaned email bodies using regex to remove replies and signatures.\n",
    "- Implemented chunking strategies:\n",
    "  - Fixed-size chunking (512 tokens with 50-token overlap)\n",
    "  - Email-level and thread-level chunking\n",
    "- Chose `all-MiniLM-L6-v2` from SentenceTransformers for fast, accurate embedding.\n",
    "- Stored vector representations in a **ChromaDB** vector database with full metadata.\n",
    "\n",
    "**Highlights**:\n",
    "- Efficient processing of ~20k emails into 50k+ chunks.\n",
    "- Each chunk is linked to metadata such as subject, sender, and thread ID.\n",
    "\n",
    "---\n",
    "\n",
    "### 2. 🔍 Search Layer\n",
    "\n",
    "**Purpose**: Retrieve the most relevant chunks based on semantic similarity to a user query.\n",
    "\n",
    "**Workflow**:\n",
    "- Embed incoming query using the same model.\n",
    "- Search ChromaDB for top-k similar chunks.\n",
    "- Implemented a **local cache** to avoid redundant embedding/search calls.\n",
    "- Applied **re-ranking** using `cross-encoder/ms-marco-MiniLM-L-6-v2` to improve relevance.\n",
    "\n",
    "**Experiments**:\n",
    "| Parameter       | Tried Values               | Result                      |\n",
    "|-----------------|----------------------------|-----------------------------|\n",
    "| Top-k           | 3, 5, 10                    | 3 worked best with reranking|\n",
    "| Re-rank Models  | STSB, MS MARCO              | MS MARCO had higher relevance|\n",
    "| Cache Format    | JSON File                   | Fast & portable              |\n",
    "\n",
    "---\n",
    "\n",
    "### 3. 🧠 Generation Layer\n",
    "\n",
    "**Purpose**: Generate an accurate, summarized response using LLM based on retrieved context.\n",
    "\n",
    "**Components**:\n",
    "- Designed a detailed prompt template to include:\n",
    "  - Instructions\n",
    "  - User query\n",
    "  - Top chunks as context\n",
    "  - Optionally few-shot examples to improve accuracy\n",
    "- Used OpenAI’s `gpt-3.5-turbo` and `gpt-4` for comparison\n",
    "- Tuned prompt to encourage specific, decision-oriented summaries\n",
    "\n",
    "**Sample Prompt Structure**:\n",
    "You are an assistant that summarizes and extracts insights from corporate email threads.\n",
    "\n",
    "Query: What was the decision on the budget?\n",
    "\n",
    "Context:\n",
    "\n",
    "[Chunk 1]\n",
    "\n",
    "[Chunk 2]\n",
    "\n",
    "[Chunk 3]\n",
    "\n",
    "Answer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acd4e1cf-c566-4ad9-a97a-c8597dcbd492",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "---\n",
    "\n",
    "## 🧪 Evaluation Queries\n",
    "\n",
    "We designed 3 realistic queries based on content from the dataset:\n",
    "\n",
    "1. **Query 1**: What summary does the thread provide about delays in project delivery?\n",
    "2. **Query 2**: What decision was made about budget increase in email thread about resource allocation?\n",
    "3. **Query 3**: What strategy was proposed in thread_id 100 regarding risk management?\n",
    "\n",
    "Each query was passed through the system and used to capture both:\n",
    "- Top 3 search results (retrieved chunks)\n",
    "- Final LLM-generated response\n",
    "\n",
    "---\n",
    "\n",
    "## 🖼️ Screenshot Deliverables\n",
    "\n",
    "### A. Top 3 Chunks from Search Layer\n",
    "\n",
    "| Query | Screenshot File                                  |\n",
    "|-------|--------------------------------------------------|\n",
    "| Q1    | `outputs/search_screenshots/query1_top3.png`     |\n",
    "| Q2    | `outputs/search_screenshots/query2_top3.png`     |\n",
    "| Q3    | `outputs/search_screenshots/query3_top3.png`     |\n",
    "\n",
    "### B. Final LLM Response from Generation Layer\n",
    "\n",
    "| Query | Screenshot File                                  |\n",
    "|-------|--------------------------------------------------|\n",
    "| Q1    | `outputs/generation_screenshots/query1_answer.png` |\n",
    "| Q2    | `outputs/generation_screenshots/query2_answer.png` |\n",
    "| Q3    | `outputs/generation_screenshots/query3_answer.png` |\n",
    "\n",
    "---\n",
    "\n",
    "## 🔬 Experimental Insights\n",
    "\n",
    "| Component        | Option Tested                          | Outcome                                    |\n",
    "|------------------|-----------------------------------------|---------------------------------------------|\n",
    "| Embedding Model | MiniLM, MPNet, OpenAI Embeddings        | MiniLM had best trade-off speed vs quality |\n",
    "| Chunking        | Fixed, thread-level, semantic           | Fixed 512 with overlap performed best      |\n",
    "| Re-ranking      | MS MARCO, STSB                          | MS MARCO improved result precision         |\n",
    "| Few-shot Prompt | Enabled/Disabled                        | Enabled yielded more accurate outputs      |\n",
    "| LLM Model       | GPT-3.5-turbo vs GPT-4                  | GPT-4 gave slightly more factual answers   |\n",
    "\n",
    "---\n",
    "\n",
    "## ⚠️ Challenges & Limitations\n",
    "\n",
    "- Thread segmentation required experimentation to avoid context fragmentation.\n",
    "- LLMs sometimes hallucinate decisions not present in the emails.\n",
    "- Embedding 50k+ chunks requires careful resource management.\n",
    "- Certain short threads provide too little context for good summaries.\n",
    "\n",
    "---\n",
    "\n",
    "## ✅ Final Thoughts\n",
    "\n",
    "Email Search AI is a robust, modular system that combines vector search with LLMs to provide high-quality retrieval and summarization for organizational emails. This can be extended to internal document search, compliance tracking, and knowledge extraction tasks.\n",
    "\n",
    "### ✅ Features Recap:\n",
    "- Semantic vector search using SentenceTransformers\n",
    "- Re-ranking using state-of-the-art cross-encoders\n",
    "- Generative answers using GPT models\n",
    "- Prompt customization with few-shot support\n",
    "- Full caching layer for performance\n",
    "\n",
    "---\n",
    "\n",
    "## 🧷 Future Enhancements\n",
    "\n",
    "- Integrate UI (e.g., Streamlit or Gradio) for business use\n",
    "- Add named-entity-based filtering for people/orgs\n",
    "- Support multi-thread summarization\n",
    "- Enable time-aware ranking of threads\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6f5d6bd-f845-4bf9-8b91-21799567ff0e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b0aff76-0e51-43ec-b0eb-915a2ca57272",
   "metadata": {},
   "outputs": [],
   "source": [
    "import streamlit as st\n",
    "from src.embedding_layer import EmbeddingProcessor\n",
    "from src.search_layer import SearchEngine\n",
    "from src.generation_layer import generate_answer\n",
    "\n",
    "# Initialize components (you might want to load embeddings & DB once if big)\n",
    "@st.cache_resource(show_spinner=False)\n",
    "def init_system():\n",
    "    embedder = EmbeddingProcessor()\n",
    "    search_engine = SearchEngine()\n",
    "    return embedder, search_engine\n",
    "\n",
    "embedder, search_engine = init_system()\n",
    "\n",
    "st.title(\"📬 Email Search AI\")\n",
    "st.markdown(\"\"\"\n",
    "Enter your query below to search across the email threads and get a summarized answer.\n",
    "\"\"\")\n",
    "\n",
    "query = st.text_input(\"Enter your query:\", \"\")\n",
    "\n",
    "TOP_K = 3\n",
    "\n",
    "if query:\n",
    "    with st.spinner(\"Searching for relevant emails...\"):\n",
    "        top_chunks = search_engine.search(query, top_k=TOP_K)\n",
    "\n",
    "    if not top_chunks:\n",
    "        st.warning(\"No relevant results found.\")\n",
    "    else:\n",
    "        st.subheader(\"Top 3 Retrieved Chunks\")\n",
    "        for i, chunk in enumerate(top_chunks):\n",
    "            st.markdown(f\"**Chunk {i+1}**\")\n",
    "            st.write(chunk['chunk'])\n",
    "            st.caption(f\"Metadata: {chunk['metadata']}\")\n",
    "\n",
    "        if st.button(\"Generate Answer\"):\n",
    "            with st.spinner(\"Generating answer from LLM...\"):\n",
    "                answer = generate_answer(query, top_chunks)\n",
    "            st.subheader(\"Generated Answer\")\n",
    "            st.write(answer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d105cf5-4bf1-40f8-9411-fd30012c6b64",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install streamlit\n",
    "streamlit run app.py\n",
    "Visit http://localhost:8501 in your browser."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e087b31d-2627-4a3d-bfe4-8623fac3892a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import streamlit as st\n",
    "from datetime import datetime\n",
    "from src.embedding_layer import EmbeddingProcessor\n",
    "from src.search_layer import SearchEngine\n",
    "from src.generation_layer import generate_answer\n",
    "\n",
    "# Initialize once per session\n",
    "@st.cache_resource(show_spinner=False)\n",
    "def init_system():\n",
    "    embedder = EmbeddingProcessor()\n",
    "    search_engine = SearchEngine()\n",
    "    return embedder, search_engine\n",
    "\n",
    "embedder, search_engine = init_system()\n",
    "\n",
    "st.title(\"📬 Email Search AI\")\n",
    "\n",
    "st.markdown(\"\"\"\n",
    "Enter your query and optionally filter email threads by date range, sender, or subject keyword.\n",
    "You can also view your past queries and generated answers.\n",
    "\"\"\")\n",
    "\n",
    "# Sidebar filters\n",
    "st.sidebar.header(\"Filters\")\n",
    "date_start = st.sidebar.date_input(\"Start Date\", value=None)\n",
    "date_end = st.sidebar.date_input(\"End Date\", value=None)\n",
    "sender_filter = st.sidebar.text_input(\"Sender Email Contains\")\n",
    "subject_filter = st.sidebar.text_input(\"Subject Contains\")\n",
    "\n",
    "query = st.text_input(\"Enter your query:\", \"\")\n",
    "\n",
    "# Session state to hold history: list of dicts {query, top_chunks, answer}\n",
    "if 'history' not in st.session_state:\n",
    "    st.session_state['history'] = []\n",
    "\n",
    "TOP_K = st.sidebar.slider(\"Number of Results to Retrieve\", min_value=3, max_value=10, value=3)\n",
    "\n",
    "def filter_chunks(chunks):\n",
    "    \"\"\"Filter chunks by metadata if filter applied.\"\"\"\n",
    "    filtered = []\n",
    "    for chunk in chunks:\n",
    "        md = chunk['metadata']\n",
    "        # date filtering\n",
    "        if date_start and 'timestamp' in md:\n",
    "            dt = datetime.strptime(md['timestamp'], \"%Y-%m-%d %H:%M:%S\")\n",
    "            if dt.date() < date_start:\n",
    "                continue\n",
    "        if date_end and 'timestamp' in md:\n",
    "            dt = datetime.strptime(md['timestamp'], \"%Y-%m-%d %H:%M:%S\")\n",
    "            if dt.date() > date_end:\n",
    "                continue\n",
    "        # sender filter\n",
    "        if sender_filter and 'from' in md:\n",
    "            if sender_filter.lower() not in md['from'].lower():\n",
    "                continue\n",
    "        # subject filter\n",
    "        if subject_filter and 'subject' in md:\n",
    "            if subject_filter.lower() not in md['subject'].lower():\n",
    "                continue\n",
    "        filtered.append(chunk)\n",
    "    return filtered\n",
    "\n",
    "if query:\n",
    "    with st.spinner(\"Searching for relevant emails...\"):\n",
    "        top_chunks = search_engine.search(query, top_k=TOP_K)\n",
    "        top_chunks = filter_chunks(top_chunks)\n",
    "\n",
    "    if not top_chunks:\n",
    "        st.warning(\"No relevant results found after applying filters.\")\n",
    "    else:\n",
    "        st.subheader(f\"Top {len(top_chunks)} Retrieved Chunks\")\n",
    "        for i, chunk in enumerate(top_chunks):\n",
    "            with st.expander(f\"Chunk {i+1} - Subject: {chunk['metadata'].get('subject', 'N/A')}\"):\n",
    "                st.write(chunk['chunk'])\n",
    "                st.caption(f\"From: {chunk['metadata'].get('from', 'N/A')} | Date: {chunk['metadata'].get('timestamp', 'N/A')}\")\n",
    "\n",
    "        if st.button(\"Generate Answer\"):\n",
    "            with st.spinner(\"Generating answer from LLM...\"):\n",
    "                answer = generate_answer(query, top_chunks)\n",
    "            st.subheader(\"Generated Answer\")\n",
    "            st.write(answer)\n",
    "            # Save to history\n",
    "            st.session_state['history'].append({\n",
    "                'query': query,\n",
    "                'top_chunks': top_chunks,\n",
    "                'answer': answer,\n",
    "                'timestamp': datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "            })\n",
    "\n",
    "# Show past queries & answers\n",
    "if st.session_state['history']:\n",
    "    st.sidebar.header(\"Query History\")\n",
    "    for i, item in enumerate(reversed(st.session_state['history'])):\n",
    "        if st.sidebar.button(f\"{item['timestamp']} - {item['query']}\", key=f\"hist_{i}\"):\n",
    "            st.write(f\"### Previous Query: {item['query']}\")\n",
    "            st.write(f\"**Answer:** {item['answer']}\")\n",
    "            st.write(\"**Top Retrieved Chunks:**\")\n",
    "            for j, chunk in enumerate(item['top_chunks']):\n",
    "                with st.expander(f\"Chunk {j+1} - Subject: {chunk['metadata'].get('subject', 'N/A')}\"):\n",
    "                    st.write(chunk['chunk'])\n",
    "                    st.caption(f\"From: {chunk['metadata'].get('from', 'N/A')} | Date: {chunk['metadata'].get('timestamp', 'N/A')}\")\n",
    "            st.markdown(\"---\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6f60a4e-3991-4349-9a9e-4dbef4151984",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b70508c5-b0ac-46da-b2a2-2de12e9247ee",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "93694aea-db21-4607-87be-b9e3246ea6b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Files saved to 'email_dataset/email_threads.csv' and 'email_summaries.csv'\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "# Sample data for email_threads.csv\n",
    "email_threads_data = [\n",
    "    [1001, \"Project Falcon Delay\", \"alice@example.com\", \"bob@example.com\", \"2025-07-01 09:15:00\",\n",
    "     \"Hi Bob,\\nWe are experiencing delays in Project Falcon due to supplier issues. Expect 2-week delay.\\nRegards,\\nAlice\"],\n",
    "    [1001, \"Project Falcon Delay\", \"bob@example.com\", \"alice@example.com\", \"2025-07-01 10:00:00\",\n",
    "     \"Thanks for the update. Can you send revised timeline?\\nThanks,\\nBob\"],\n",
    "    [1001, \"Project Falcon Delay\", \"alice@example.com\", \"bob@example.com\", \"2025-07-01 11:30:00\",\n",
    "     \"Sure. Revised delivery expected by July 14th.\\n-Alice\"],\n",
    "    [1002, \"Q3 Marketing Budget\", \"carol@example.com\", \"team@example.com\", \"2025-06-15 14:30:00\",\n",
    "     \"Finance approved 10% increase in marketing for Q3.\\nPlease adjust campaigns accordingly.\\nCarol\"],\n",
    "    [1002, \"Q3 Marketing Budget\", \"dave@example.com\", \"team@example.com\", \"2025-06-15 15:00:00\",\n",
    "     \"Thanks Carol. Please proceed accordingly.\\n-Dave\"],\n",
    "    [1003, \"Office Move Update\", \"john@example.com\", \"staff@example.com\", \"2025-07-20 08:00:00\",\n",
    "     \"We’re moving to the 5th floor starting next Monday. Packing materials will be provided.\\n-John\"],\n",
    "    [1003, \"Office Move Update\", \"sara@example.com\", \"john@example.com\", \"2025-07-20 08:45:00\",\n",
    "     \"Will the network be available during the move?\\n-Sara\"],\n",
    "    [1003, \"Office Move Update\", \"john@example.com\", \"sara@example.com\", \"2025-07-20 09:15:00\",\n",
    "     \"Yes, minimal downtime expected.\\n-John\"],\n",
    "    [1004, \"New Intern Joining\", \"recruiter@example.com\", \"hr@example.com\", \"2025-08-01 12:00:00\",\n",
    "     \"Kavita Sharma will join as intern in product team starting 5th Aug.\\nRecruiter\"],\n",
    "    [1004, \"New Intern Joining\", \"hr@example.com\", \"recruiter@example.com\", \"2025-08-01 12:15:00\",\n",
    "     \"Noted. Welcome email and onboarding to be scheduled.\\n-HR\"],\n",
    "    [1005, \"Client Feedback - Zenith Corp\", \"sales@example.com\", \"product@example.com\", \"2025-07-25 16:00:00\",\n",
    "     \"Zenith reported confusion around analytics dashboard. Suggested improvements:\\n- Add tooltips\\n- Better export options\"],\n",
    "    [1005, \"Client Feedback - Zenith Corp\", \"product@example.com\", \"sales@example.com\", \"2025-07-25 16:30:00\",\n",
    "     \"Thanks! Adding these to the next sprint planning.\\n-Product Team\"],\n",
    "    [1006, \"Security Training Reminder\", \"it@example.com\", \"staff@example.com\", \"2025-08-01 09:00:00\",\n",
    "     \"Reminder: Security awareness training is mandatory. Complete by Aug 10th.\\n-IT Team\"],\n",
    "    [1006, \"Security Training Reminder\", \"sam@example.com\", \"it@example.com\", \"2025-08-01 09:15:00\",\n",
    "     \"Got it, will complete by deadline.\\n-Sam\"],\n",
    "    [1007, \"Leave Approval - Tanya\", \"manager@example.com\", \"hr@example.com\", \"2025-07-10 11:00:00\",\n",
    "     \"Tanya's leave from July 18 to 22 is approved.\\n-Manager\"],\n",
    "    [1007, \"Leave Approval - Tanya\", \"hr@example.com\", \"manager@example.com\", \"2025-07-10 11:15:00\",\n",
    "     \"Thanks. We'll update the system.\\n-HR\"],\n",
    "    [1008, \"Procurement Request Approval\", \"admin@example.com\", \"finance@example.com\", \"2025-07-28 10:00:00\",\n",
    "     \"Requesting approval for purchasing new laptops for dev team.\\nCost: ₹3,00,000.\\n-Admin\"],\n",
    "    [1008, \"Procurement Request Approval\", \"finance@example.com\", \"admin@example.com\", \"2025-07-28 10:30:00\",\n",
    "     \"Approved. Proceed with procurement.\\n-Finance\"],\n",
    "    [1009, \"Remote Work Extension Request\", \"tina@example.com\", \"manager@example.com\", \"2025-07-12 09:30:00\",\n",
    "     \"Requesting remote work extension till July 31 due to personal reasons.\\n-Tina\"],\n",
    "    [1009, \"Remote Work Extension Request\", \"manager@example.com\", \"tina@example.com\", \"2025-07-12 10:00:00\",\n",
    "     \"Extension granted. Keep in touch with team lead daily.\\n-Manager\"],\n",
    "    [1010, \"Expense Report Clarification\", \"employee@example.com\", \"accounts@example.com\", \"2025-08-01 14:00:00\",\n",
    "     \"Please clarify why July travel expenses were partially reimbursed.\\n-Employee\"],\n",
    "    [1010, \"Expense Report Clarification\", \"accounts@example.com\", \"employee@example.com\", \"2025-08-01 14:30:00\",\n",
    "     \"Some receipts were missing. Please upload them to get full reimbursement.\\n-Accounts\"],\n",
    "]\n",
    "\n",
    "# Sample data for email_summaries.csv\n",
    "email_summaries_data = [\n",
    "    [1001, \"Project Falcon is delayed by two weeks due to supplier issues. Revised delivery is on July 14th.\"],\n",
    "    [1002, \"Finance approved a 10% increase in the Q3 marketing budget. Campaigns will be adjusted accordingly.\"],\n",
    "    [1003, \"The office will move to the 5th floor next Monday. Network will remain mostly available during the transition.\"],\n",
    "    [1004, \"Kavita Sharma is joining as a product intern on August 5th. Onboarding will be scheduled by HR.\"],\n",
    "    [1005, \"Zenith Corp provided feedback requesting improvements to the analytics dashboard, including tooltips and export options.\"],\n",
    "    [1006, \"Security training is mandatory and must be completed by August 10th.\"],\n",
    "    [1007, \"Tanya's leave from July 18 to 22 was approved and HR will update the records.\"],\n",
    "    [1008, \"The finance team approved a laptop procurement request worth ₹3,00,000 for the development team.\"],\n",
    "    [1009, \"Tina's remote work request was extended until July 31. She is expected to check in daily.\"],\n",
    "    [1010, \"July travel expenses were partially reimbursed due to missing receipts. Full reimbursement will be made after uploading them.\"]\n",
    "]\n",
    "\n",
    "# Convert to DataFrames\n",
    "threads_df = pd.DataFrame(email_threads_data, columns=[\"thread_id\", \"subject\", \"from\", \"to\", \"timestamp\", \"body\"])\n",
    "summaries_df = pd.DataFrame(email_summaries_data, columns=[\"thread_id\", \"summary\"])\n",
    "\n",
    "# Save to CSV\n",
    "os.makedirs(\"email_dataset\", exist_ok=True)\n",
    "threads_df.to_csv(\"email_dataset/email_threads.csv\", index=False)\n",
    "summaries_df.to_csv(\"email_dataset/email_summaries.csv\", index=False)\n",
    "\n",
    "print(\"✅ Files saved to 'email_dataset/email_threads.csv' and 'email_summaries.csv'\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a0d130f-fe82-4b48-848f-dd7c4aa62a16",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcfb2022-12f9-4ce9-a4e3-f197f3439de6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# Define directory structure\n",
    "dirs = [\n",
    "    \"email-search-ai/data\",\n",
    "    \"email-search-ai/src\",\n",
    "    \"email-search-ai/outputs/search_screenshots\",\n",
    "    \"email-search-ai/outputs/generation_screenshots\",\n",
    "    \"email-search-ai/docs\"\n",
    "]\n",
    "\n",
    "# Define files to create\n",
    "files = {\n",
    "    \"email-search-ai/src/embedding_layer.py\": \"\",\n",
    "    \"email-search-ai/src/search_layer.py\": \"\",\n",
    "    \"email-search-ai/src/generation_layer.py\": \"\",\n",
    "    \"email-search-ai/src/cache.py\": \"\",\n",
    "    \"email-search-ai/src/utils.py\": \"\",\n",
    "    \"email-search-ai/main.py\": \"\",\n",
    "    \"email-search-ai/requirements.txt\": \"\",\n",
    "    \"email-search-ai/docs/project_documentation.md\": \"\"\n",
    "}\n",
    "\n",
    "# Create directories\n",
    "for dir_path in dirs:\n",
    "    os.makedirs(dir_path, exist_ok=True)\n",
    "\n",
    "# Create empty files\n",
    "for file_path in files:\n",
    "    with open(file_path, \"w\") as f:\n",
    "        f.write(files[file_path])\n",
    "\n",
    "print(\"✅ Project structure and base files created!\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
