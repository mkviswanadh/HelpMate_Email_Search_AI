{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e33ce0dd-d50f-4ff4-b441-8c0ca112da2a",
   "metadata": {},
   "source": [
    "# Email Search AI"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85dfb6d5-861d-4823-bbcc-cf7283266703",
   "metadata": {},
   "source": [
    "<img src=\"email_search_ai.png\" style=\"display:block; margin-left:auto; margin-right:auto;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa3ac6d9-d4dc-4768-9908-98ebf8237e79",
   "metadata": {},
   "source": [
    "## Problem Statement"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7393f3d8-c376-47fc-9b5e-26d5aae0b123",
   "metadata": {},
   "source": [
    "In enterprise environments, email threads often contain critical discussions, decisions, and context across multiple stakeholders. However, locating specific information within large, unstructured, and nested email threads is time-consuming and inefficient using conventional keyword-based search tools. Professionals face challenges in extracting relevant insights without wading through entire conversations manually. There is a pressing need for an intelligent system that enables **semantic search** and **automated summarization** of email threads."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e6b6599-9758-44a5-903b-2c17c91771e1",
   "metadata": {},
   "source": [
    "## Project Overview"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d852373-9aa4-497f-bf0a-65fef70f3d74",
   "metadata": {},
   "source": [
    "**HelpMate_Email Search_AI** is an end-to-end **RAG-based AI assistant** designed for semantic search and summarization of email threads. It uses **Sentence Transformers** for creating dense **vector embeddings** of email content, stores them in a **ChromaDB vector store**, and enables **semantic index search**. Upon receiving a user query, it retrieves the most relevant chunks using **vector similarity**, improves result quality through **cross-encoder-based reranking**, and finally, generates context-aware responses using **OpenAI's LLM (e.g., GPT-3.5-turbo)**.\n",
    "\n",
    "The system employs a **Retrieval-Augmented Generation (RAG)** architecture with **caching** for efficiency and performance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a00a5e8-8114-41e2-b546-826bcd18ef38",
   "metadata": {},
   "source": [
    "## Project Objectives"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72b5d0f1-0f93-4758-81d4-a9ab82a9f3c5",
   "metadata": {},
   "source": [
    "- **Semantic Understanding of Emails**: Use **Sentence Transformers (all-MiniLM-L6-v2)** to convert email chunks into vector representations capturing semantic meaning.\n",
    "\n",
    "- **Vector Database Indexing**: Store email embeddings in **ChromaDB**, enabling fast approximate nearest neighbor (ANN) vector search.\n",
    "\n",
    "- **Semantic Search & Retrieval**: Support user queries via **embedding-based similarity search** across indexed email chunks.\n",
    "\n",
    "- **Result Reranking**: Improve retrieval accuracy with **cross-encoder reranking (ms-marco-MiniLM-L-6-v2)**, scoring relevance between query and result pairs.\n",
    "\n",
    "- **Contextual Answer Generation**: Use a **Retrieval-Augmented Generation (RAG)** pipeline to feed retrieved results into OpenAI GPT models for answer synthesis.\n",
    "\n",
    "- **Query Caching**: Implement a file-based **caching** layer to store query results and avoid repeated computations."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "718433c1-8f5a-4922-8b14-b610a63373da",
   "metadata": {},
   "source": [
    "## Functional Features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e446c9c1-cd6f-4b10-b0aa-866cc72ea437",
   "metadata": {},
   "source": [
    "\n",
    "| **Component**                   | **Description**                                                                 | **Technology Used**                             |\n",
    "|--------------------------------|---------------------------------------------------------------------------------|-------------------------------------------------|\n",
    "| **Email Preprocessing**        | Cleans raw email bodies by removing quoted replies and normalizing text         | `Regex`, custom cleaning                        |\n",
    "| **Chunking**                   | Splits cleaned emails into overlapping token-limited chunks                     | Custom logic                                    |\n",
    "| **Embeddings**                 | Transforms email chunks into dense semantic vectors                             | `SentenceTransformer` (`all-MiniLM-L6-v2`)      |\n",
    "| **Vector Indexing**            | Stores and indexes embeddings for fast similarity search                        | `ChromaDB`                                      |\n",
    "| **Query Embedding**            | Embeds natural language queries for semantic comparison                         | `SentenceTransformer`                           |\n",
    "| **Initial Vector Search**      | Retrieves top-N similar email chunks using ANN search                           | `ChromaDB`                                      |\n",
    "| **Reranking**                  | Reorders retrieved results by true semantic relevance                           | `CrossEncoder` (`ms-marco-MiniLM-L-6-v2`)       |\n",
    "| **Retrieval-Augmented Generation (RAG)** | Combines retrieved chunks with the query to form a prompt for GPT      | `OpenAI GPT-3.5-turbo`                          |\n",
    "| **Answer Generation**          | Synthesizes a coherent answer based on context                                  | `OpenAI Chat Completion API`                    |\n",
    "| **Caching**                    | Stores query results using hashed query keys for faster repeat access           | JSON file-based custom `Cache` class            |\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bfd8630-3e52-4149-9a29-ee1bf4b6e6ff",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "db4fb8c1-e9cf-4c4f-b3be-15326f8101e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Admin\\anaconda3\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "# Import the libraries\n",
    "import os\n",
    "import re\n",
    "import pandas as pd\n",
    "from typing import List, Dict\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import chromadb\n",
    "from chromadb.config import Settings\n",
    "from chromadb.utils import embedding_functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f5017b7-6c7e-4712-8c6a-49172d053321",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Install required modules\n",
    "!pip3 install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3bd25ece-0ec2-45ff-93cc-4522847a9793",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.95.1\n"
     ]
    }
   ],
   "source": [
    "#Let's check the version of OpenAI\n",
    "import openai\n",
    "print(openai.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "79dacc8b-d521-41a1-9d26-92777ee18fbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read key from text file\n",
    "with open(\"openai_key.txt\", \"r\") as f:\n",
    "    api_key = f.read().strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "12cf2b9a-5fa0-48dc-8ab0-5d1b9be4a007",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pass key to OpenAI client\n",
    "from openai import OpenAI\n",
    "client = OpenAI(api_key=api_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "80bfa77e-effa-4aec-a400-5583335af460",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>thread_id</th>\n",
       "      <th>subject</th>\n",
       "      <th>from</th>\n",
       "      <th>to</th>\n",
       "      <th>timestamp</th>\n",
       "      <th>body</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1001</td>\n",
       "      <td>Project Falcon Delay</td>\n",
       "      <td>alice@example.com</td>\n",
       "      <td>bob@example.com</td>\n",
       "      <td>2025-07-01 09:15:00</td>\n",
       "      <td>Hi Bob,\\nWe are experiencing delays in Project...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1001</td>\n",
       "      <td>Project Falcon Delay</td>\n",
       "      <td>bob@example.com</td>\n",
       "      <td>alice@example.com</td>\n",
       "      <td>2025-07-01 10:00:00</td>\n",
       "      <td>Thanks for the update. Can you send revised ti...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1001</td>\n",
       "      <td>Project Falcon Delay</td>\n",
       "      <td>alice@example.com</td>\n",
       "      <td>bob@example.com</td>\n",
       "      <td>2025-07-01 11:30:00</td>\n",
       "      <td>Sure. Revised delivery expected by July 14th.\\...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1002</td>\n",
       "      <td>Q3 Marketing Budget</td>\n",
       "      <td>carol@example.com</td>\n",
       "      <td>team@example.com</td>\n",
       "      <td>2025-06-15 14:30:00</td>\n",
       "      <td>Finance approved 10% increase in marketing for...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1002</td>\n",
       "      <td>Q3 Marketing Budget</td>\n",
       "      <td>dave@example.com</td>\n",
       "      <td>team@example.com</td>\n",
       "      <td>2025-06-15 15:00:00</td>\n",
       "      <td>Thanks Carol. Please proceed accordingly.\\n-Dave</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   thread_id               subject               from                 to  \\\n",
       "0       1001  Project Falcon Delay  alice@example.com    bob@example.com   \n",
       "1       1001  Project Falcon Delay    bob@example.com  alice@example.com   \n",
       "2       1001  Project Falcon Delay  alice@example.com    bob@example.com   \n",
       "3       1002   Q3 Marketing Budget  carol@example.com   team@example.com   \n",
       "4       1002   Q3 Marketing Budget   dave@example.com   team@example.com   \n",
       "\n",
       "             timestamp                                               body  \n",
       "0  2025-07-01 09:15:00  Hi Bob,\\nWe are experiencing delays in Project...  \n",
       "1  2025-07-01 10:00:00  Thanks for the update. Can you send revised ti...  \n",
       "2  2025-07-01 11:30:00  Sure. Revised delivery expected by July 14th.\\...  \n",
       "3  2025-06-15 14:30:00  Finance approved 10% increase in marketing for...  \n",
       "4  2025-06-15 15:00:00   Thanks Carol. Please proceed accordingly.\\n-Dave  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Read the input dataset\n",
    "df_email_thread = pd.read_csv(\"email_dataset/email_threads.csv\")\n",
    "df_email_thread.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e3f38fd1-49e4-48e9-a432-4c882b3b30a6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 22 entries, 0 to 21\n",
      "Data columns (total 6 columns):\n",
      " #   Column     Non-Null Count  Dtype \n",
      "---  ------     --------------  ----- \n",
      " 0   thread_id  22 non-null     int64 \n",
      " 1   subject    22 non-null     object\n",
      " 2   from       22 non-null     object\n",
      " 3   to         22 non-null     object\n",
      " 4   timestamp  22 non-null     object\n",
      " 5   body       22 non-null     object\n",
      "dtypes: int64(1), object(5)\n",
      "memory usage: 1.2+ KB\n"
     ]
    }
   ],
   "source": [
    "df_email_thread.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bb7a67a-6a86-402a-9762-a4588c735600",
   "metadata": {},
   "source": [
    "**Description**:\n",
    "The email_thread_details file provides a detailed perspective on individual email threads, encompassing crucial information such as subject, timestamp, sender, recipients, and the content of the email.\n",
    "\n",
    "**Columns**:\n",
    "- **thread_id**: A unique identifier for each email thread.\n",
    "- **subject**: Subject of the email thread.\n",
    "- **timestamp**: Timestamp indicating when the message was sent.\n",
    "- **from**: Sender of the email.\n",
    "- **to**: List of recipients of the email.\n",
    "- **body**: Content of the email message."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ac00b3c-6dfb-4f9a-a2a4-d6d0c128abd1",
   "metadata": {},
   "source": [
    "## Overall strcture of the code"
   ]
  },
  {
   "cell_type": "raw",
   "id": "04f964b4-ebeb-4dc0-9ad9-082c38ccc501",
   "metadata": {},
   "source": [
    "email-search-ai/\n",
    "│\n",
    "├── email_dataset/\n",
    "│   └── email_thread_summary.csv \n",
    "│\n",
    "├── src/\n",
    "│   ├── embedding_layer.py\n",
    "│   ├── search_layer.py\n",
    "│   ├── cache.py\n",
    "│   ├── generation_layer.py\n",
    "│   └── utils.py\n",
    "│\n",
    "├── outputs/\n",
    "│   ├── search_screenshots/\n",
    "│   ├── generation_screenshots/\n",
    "│\n",
    "├── docs/\n",
    "│   └── project_documentation.md\n",
    "│\n",
    "├── main.py\n",
    "└── requirements.txt\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "123c8270-d708-427c-8907-31683ae8e041",
   "metadata": {},
   "source": [
    "## Embedding Layer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c4b9ef0-ed2a-4c33-a25c-8812539f66fc",
   "metadata": {},
   "source": [
    "<img src=\"embedding_layer_flow.jpg\" style=\"display:block; margin-left:auto; margin-right:auto;\"/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8c77d2e3-b28f-46a1-a4bd-9852cef8a2df",
   "metadata": {},
   "outputs": [],
   "source": [
    "# src/embedding_layer.py\n",
    "\n",
    "# Import the libraries\n",
    "import os\n",
    "import re\n",
    "import pandas as pd\n",
    "from typing import List, Dict\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import chromadb\n",
    "from chromadb.config import Settings\n",
    "from chromadb.utils import embedding_functions\n",
    "\n",
    "# === CLEANING FUNCTIONS ===\n",
    "\n",
    "def clean_email_body(body: str) -> str:\n",
    "    body = re.sub(r'[\\r\\n]+', ' ', body)  # remove newlines\n",
    "    body = re.sub(r'\\s+', ' ', body)  # normalize spaces\n",
    "    body = re.sub(r'On .* wrote:', '', body)  # remove quoted replies\n",
    "    return body.strip()\n",
    "\n",
    "\n",
    "# === CHUNKING STRATEGY ===\n",
    "\n",
    "def chunk_text(text: str, max_tokens: int = 512, overlap: int = 50) -> List[str]:\n",
    "    words = text.split()\n",
    "    chunks = []\n",
    "    for i in range(0, len(words), max_tokens - overlap):\n",
    "        chunk = ' '.join(words[i:i + max_tokens])\n",
    "        if len(chunk.split()) > 10:\n",
    "            chunks.append(chunk)\n",
    "    return chunks\n",
    "\n",
    "\n",
    "# === EMBEDDING + CHROMA ===\n",
    "\n",
    "class EmbeddingProcessor:\n",
    "    def __init__(self, client, chroma_path: str = \"chroma_db\", model_name: str = \"all-MiniLM-L6-v2\"):\n",
    "        self.model = SentenceTransformer(model_name)\n",
    "        self.client = client\n",
    "        self.chroma_collection = self.client.get_or_create_collection(name=\"email_chunks\")\n",
    "\n",
    "    def process_emails(self, df: pd.DataFrame):\n",
    "        all_chunks = []\n",
    "        metadatas = []\n",
    "\n",
    "        for idx, row in df.iterrows():\n",
    "            cleaned_body = clean_email_body(row['body'])\n",
    "            chunks = chunk_text(cleaned_body)\n",
    "\n",
    "            for i, chunk in enumerate(chunks):\n",
    "                chunk_id = f\"{row['thread_id']}_{idx}_{i}\"\n",
    "                all_chunks.append({\n",
    "                    \"id\": chunk_id,\n",
    "                    \"text\": chunk,\n",
    "                    \"metadata\": {\n",
    "                        \"thread_id\": row[\"thread_id\"],\n",
    "                        \"subject\": row[\"subject\"],\n",
    "                        \"from\": row[\"from\"],\n",
    "                        \"timestamp\": row[\"timestamp\"]\n",
    "                    }\n",
    "                })\n",
    "\n",
    "        print(f\"Embedding {len(all_chunks)} chunks...\")\n",
    "        embeddings = self.model.encode([c['text'] for c in all_chunks], show_progress_bar=True).tolist()\n",
    "\n",
    "        self.chroma_collection.add(\n",
    "            documents=[c['text'] for c in all_chunks],\n",
    "            embeddings=embeddings,\n",
    "            metadatas=[c['metadata'] for c in all_chunks],\n",
    "            ids=[c['id'] for c in all_chunks]\n",
    "        )\n",
    "        \n",
    "\n",
    "    def get_collection(self):\n",
    "        return self.chroma_collection\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "229a5988-4aef-4ea3-8f88-20cbdce958aa",
   "metadata": {},
   "source": [
    "## Cache Layer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fe58e82-38a8-4d5a-a5e8-ccf71708906c",
   "metadata": {},
   "source": [
    "<img src=\"cache_layer_flow1.jpg\" style=\"display:block; margin-left:auto; margin-right:auto;\"/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "2a45265f-cc17-46db-9adc-33d9ea5b29f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# src/cache.py\n",
    "\n",
    "import json\n",
    "import os\n",
    "\n",
    "\n",
    "class Cache:\n",
    "    def __init__(self, cache_file: str):\n",
    "        self.cache_file = cache_file\n",
    "        if os.path.exists(cache_file):\n",
    "            with open(cache_file, \"r\") as f:\n",
    "                self.cache = json.load(f)\n",
    "        else:\n",
    "            self.cache = {}\n",
    "\n",
    "    def contains(self, key: str) -> bool:\n",
    "        return key in self.cache\n",
    "\n",
    "    def get(self, key: str):\n",
    "        return self.cache.get(key, None)\n",
    "\n",
    "    def set(self, key: str, value):\n",
    "        self.cache[key] = value\n",
    "        self._save()\n",
    "\n",
    "    def _save(self):\n",
    "        with open(self.cache_file, \"w\") as f:\n",
    "            json.dump(self.cache, f, indent=2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe11c0f6-29b2-43e6-b9a1-efe855d488be",
   "metadata": {},
   "source": [
    "## Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "d806d803-07f3-47db-b0f6-46e9bbcd79f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def convert_np_types(obj):\n",
    "    if isinstance(obj, dict):\n",
    "        return {k: convert_np_types(v) for k, v in obj.items()}\n",
    "    elif isinstance(obj, list):\n",
    "        return [convert_np_types(i) for i in obj]\n",
    "    elif isinstance(obj, np.float32) or isinstance(obj, np.float64):\n",
    "        return float(obj)\n",
    "    elif isinstance(obj, np.int32) or isinstance(obj, np.int64):\n",
    "        return int(obj)\n",
    "    else:\n",
    "        return obj\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19d215b6-d29d-4746-b49f-7f57fd0951f4",
   "metadata": {},
   "source": [
    "## Search Layer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "834fbdf3-bf5d-49e5-863e-0212014a6c4f",
   "metadata": {},
   "source": [
    "<img src=\"search_layer_flow.jpg\" style=\"display:block; margin-left:auto; margin-right:auto;\"/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "2cd3b0c1-c451-4be0-b8dc-4830eb944f09",
   "metadata": {},
   "outputs": [],
   "source": [
    "# src/search_layer.py\n",
    "\n",
    "import hashlib\n",
    "import json\n",
    "import os\n",
    "from typing import List, Dict\n",
    "\n",
    "import numpy as np\n",
    "from sentence_transformers import SentenceTransformer, CrossEncoder\n",
    "import chromadb\n",
    "from chromadb.config import Settings\n",
    "\n",
    "#from .cache import Cache\n",
    "\n",
    "# === CONFIGURATION ===\n",
    "CACHE_PATH = \"cache/search_cache.json\"\n",
    "CHROMA_PATH = \"chroma_db\"\n",
    "\n",
    "class SearchEngine:\n",
    "    def __init__(\n",
    "        self,\n",
    "        client,\n",
    "        embedding_model_name: str = \"all-MiniLM-L6-v2\",\n",
    "        cross_encoder_model_name: str = \"cross-encoder/ms-marco-MiniLM-L-6-v2\"\n",
    "    ):\n",
    "        self.embedder = SentenceTransformer(embedding_model_name)\n",
    "        self.reranker = CrossEncoder(cross_encoder_model_name)\n",
    "        self.cache = Cache(CACHE_PATH)\n",
    "\n",
    "        self.client = client\n",
    "        self.collection = self.client.get_or_create_collection(name=\"email_chunks\")\n",
    "\n",
    "    def embed_query(self, query: str) -> List[float]:\n",
    "        return self.embedder.encode(query).tolist()\n",
    "\n",
    "    def search(self, query: str, top_k: int = 5, filter_thread_id: int = None) -> List[Dict]:\n",
    "        query_hash = hashlib.md5(query.encode()).hexdigest()\n",
    "\n",
    "        if self.cache.contains(query_hash):\n",
    "            return self.cache.get(query_hash)\n",
    "\n",
    "        query_embedding = self.embed_query(query)\n",
    "\n",
    "        search_args = {\n",
    "                            \"query_embeddings\": [query_embedding],\n",
    "                            \"n_results\": top_k * 2,  # get more to allow for reranking\n",
    "                      }\n",
    "        \n",
    "        if filter_thread_id is not None:\n",
    "            search_args[\"where\"] = {\"thread_id\": int(filter_thread_id)}\n",
    "\n",
    "        results = self.collection.query(**search_args)\n",
    "\n",
    "        documents = results[\"documents\"][0]\n",
    "        metadatas = results[\"metadatas\"][0]\n",
    "\n",
    "        # === Re-ranking ===\n",
    "        pairs = [(query, doc) for doc in documents]\n",
    "        scores = self.reranker.predict(pairs)\n",
    "\n",
    "        reranked = sorted(zip(documents, metadatas, scores), key=lambda x: x[2], reverse=True)\n",
    "\n",
    "        top_chunks = [\n",
    "            {\"chunk\": doc, \"metadata\": meta, \"score\": score}\n",
    "            for doc, meta, score in reranked[:top_k]\n",
    "        ]\n",
    "\n",
    "        scored_chunks = convert_np_types(top_chunks)\n",
    "\n",
    "        self.cache.set(query_hash, scored_chunks)\n",
    "        return top_chunks\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b53bcc8-f41f-4401-b99b-9205f1d73dcc",
   "metadata": {},
   "source": [
    "## Generation Layer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ee0af25-fa4f-4f31-b14c-e465d93d408f",
   "metadata": {},
   "source": [
    "<img src=\"generation_layer_flow.jpg\" style=\"display:block; margin-left:auto; margin-right:auto;\"/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "1bf02612-c045-4b86-b53a-1124355b60c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# src/generation_layer.py\n",
    "\n",
    "import openai\n",
    "from typing import List, Dict\n",
    "import os\n",
    "\n",
    "# Set your OpenAI API key securely\n",
    "openai.api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "\n",
    "# === Prompt Template ===\n",
    "\n",
    "def build_prompt(query: str, chunks: List[Dict], few_shot: bool = False) -> str:\n",
    "    prompt = \"You are an assistant that summarizes and extracts insights from corporate email threads.\\n\"\n",
    "    prompt += \"Given a user query and the relevant email thread excerpts, answer the question concisely and accurately. answer the question based on what is explicitly stated in the emails\\n\\n\"\n",
    "\n",
    "    if few_shot:\n",
    "        prompt += (\n",
    "            \"Example:\\n\"\n",
    "            \"Query: What was the decision on the marketing budget for Q2?\\n\"\n",
    "            \"Context:\\n\"\n",
    "            \"- The marketing team proposed a 20% increase for digital campaigns.\\n\"\n",
    "            \"- Finance approved a 10% increase after negotiation.\\n\"\n",
    "            \"Answer: A 10% increase in the Q2 marketing budget was approved after negotiation.\\n\\n\"\n",
    "        )\n",
    "\n",
    "    prompt += f\"Query: {query}\\nContext:\\n\"\n",
    "    for idx, chunk in enumerate(chunks):\n",
    "        prompt += f\"- {chunk['chunk']}\\n\"\n",
    "    prompt += \"\\nAnswer:\"\n",
    "    return prompt\n",
    "\n",
    "\n",
    "# === Generator Function ===\n",
    "\n",
    "def generate_answer(query: str, chunks: List[Dict], model: str = \"gpt-3.5-turbo\") -> str:\n",
    "    prompt = build_prompt(query, chunks, few_shot=True)\n",
    "\n",
    "    try:\n",
    "        response = client.chat.completions.create(\n",
    "            model=model,\n",
    "            messages=[\n",
    "                {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "                {\"role\": \"user\", \"content\": prompt}\n",
    "            ],\n",
    "            temperature=0.3,\n",
    "            max_tokens=300\n",
    "        )\n",
    "        answer = response.choices[0].message.content.strip()\n",
    "        return answer\n",
    "    except Exception as e:\n",
    "        print(f\"Error calling OpenAI API: {e}\")\n",
    "        return \"Sorry, I couldn't generate a response due to an error.\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "b7ff9482-b270-4f82-9cb6-0da7b05815b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading dataset...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to send telemetry event ClientStartEvent: capture() takes 1 positional argument but 3 were given\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedding data...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to send telemetry event ClientCreateCollectionEvent: capture() takes 1 positional argument but 3 were given\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedding 10 chunks...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|██████████| 1/1 [00:01<00:00,  1.34s/it]\n",
      "Failed to send telemetry event CollectionAddEvent: capture() takes 1 positional argument but 3 were given\n",
      "Failed to send telemetry event ClientCreateCollectionEvent: capture() takes 1 positional argument but 3 were given\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== QUERY: What summary does the thread provide about delays in project delivery? ===\n",
      "\n",
      "Top Retrieved Chunks:\n",
      "\n",
      "--- Chunk 1 ---\n",
      "Hi Bob, We are experiencing delays in Project Falcon due to supplier issues. Expect 2-week delay. Regards, Alice\n",
      "Metadata: {'from': 'alice@example.com', 'subject': 'Project Falcon Delay', 'thread_id': 1001, 'timestamp': '2025-07-01 09:15:00'}\n",
      "\n",
      "--- Chunk 2 ---\n",
      "Requesting remote work extension till July 31 due to personal reasons. -Tina\n",
      "Metadata: {'from': 'tina@example.com', 'subject': 'Remote Work Extension Request', 'thread_id': 1009, 'timestamp': '2025-07-12 09:30:00'}\n",
      "\n",
      "--- Chunk 3 ---\n",
      "Thanks for the update. Can you send revised timeline? Thanks, Bob\n",
      "Metadata: {'from': 'bob@example.com', 'subject': 'Project Falcon Delay', 'thread_id': 1001, 'timestamp': '2025-07-01 10:00:00'}\n",
      "\n",
      "Generated Answer:\n",
      "Delays in Project Falcon are due to supplier issues, resulting in a 2-week delay. Tina requested a remote work extension until July 31.\n",
      "\n",
      "=== QUERY: What decision was made about budget increase in email thread about resource allocation? ===\n",
      "\n",
      "Top Retrieved Chunks:\n",
      "\n",
      "--- Chunk 1 ---\n",
      "Finance approved 10% increase in marketing for Q3. Please adjust campaigns accordingly. Carol\n",
      "Metadata: {'from': 'carol@example.com', 'subject': 'Q3 Marketing Budget', 'thread_id': 1002, 'timestamp': '2025-06-15 14:30:00'}\n",
      "\n",
      "--- Chunk 2 ---\n",
      "Thanks for the update. Can you send revised timeline? Thanks, Bob\n",
      "Metadata: {'from': 'bob@example.com', 'subject': 'Project Falcon Delay', 'thread_id': 1001, 'timestamp': '2025-07-01 10:00:00'}\n",
      "\n",
      "--- Chunk 3 ---\n",
      "Some receipts were missing. Please upload them to get full reimbursement. -Accounts\n",
      "Metadata: {'from': 'accounts@example.com', 'subject': 'Expense Report Clarification', 'thread_id': 1010, 'timestamp': '2025-08-01 14:30:00'}\n",
      "\n",
      "Generated Answer:\n",
      "A 10% increase in marketing budget for Q3 was approved by Finance.\n",
      "\n",
      "=== QUERY: What strategy was proposed in thread_id 100 regarding risk management? ===\n",
      "\n",
      "Top Retrieved Chunks:\n",
      "\n",
      "--- Chunk 1 ---\n",
      "Zenith reported confusion around analytics dashboard. Suggested improvements: - Add tooltips - Better export options\n",
      "Metadata: {'from': 'sales@example.com', 'subject': 'Client Feedback - Zenith Corp', 'thread_id': 1005, 'timestamp': '2025-07-25 16:00:00'}\n",
      "\n",
      "--- Chunk 2 ---\n",
      "Thanks for the update. Can you send revised timeline? Thanks, Bob\n",
      "Metadata: {'from': 'bob@example.com', 'subject': 'Project Falcon Delay', 'thread_id': 1001, 'timestamp': '2025-07-01 10:00:00'}\n",
      "\n",
      "--- Chunk 3 ---\n",
      "Reminder: Security awareness training is mandatory. Complete by Aug 10th. -IT Team\n",
      "Metadata: {'from': 'it@example.com', 'subject': 'Security Training Reminder', 'thread_id': 1006, 'timestamp': '2025-08-01 09:00:00'}\n",
      "\n",
      "Generated Answer:\n",
      "I'm sorry, but I don't have access to specific email threads or thread IDs to provide you with the answer to your query. If you can provide me with the relevant excerpts or details from the email thread regarding the proposed strategy for risk management, I would be happy to help summarize and extract insights for you.\n"
     ]
    }
   ],
   "source": [
    "# main.py\n",
    "\n",
    "#import pandas as pd\n",
    "#from src.embedding_layer import EmbeddingProcessor\n",
    "#from src.search_layer import SearchEngine\n",
    "#from src.generation_layer import generate_answer\n",
    "\n",
    "# === CONFIG ===\n",
    "DATA_PATH = \"email_dataset/email_threads.csv\"\n",
    "TOP_K = 3\n",
    "\n",
    "# === LOAD DATA ===\n",
    "print(\"Loading dataset...\")\n",
    "df = pd.read_csv(DATA_PATH).dropna(subset=[\"body\"])\n",
    "\n",
    "chroma_client = chromadb.Client(Settings(persist_directory=\"chroma_db\"))\n",
    "\n",
    "# === EMBEDDING PHASE ===\n",
    "print(\"Embedding data...\")\n",
    "embedder = EmbeddingProcessor(client=chroma_client)\n",
    "embedder.process_emails(df)\n",
    "\n",
    "# === SEARCH PHASE ===\n",
    "search_engine = SearchEngine(client=chroma_client)\n",
    "\n",
    "# === QUERIES TO TEST ===\n",
    "queries = [\n",
    "    \"What summary does the thread provide about delays in project delivery?\",\n",
    "    \"What decision was made about budget increase in email thread about resource allocation?\",\n",
    "    \"What strategy was proposed in thread_id 100 regarding risk management?\"\n",
    "]\n",
    "\n",
    "# === RUN PIPELINE ===\n",
    "for query in queries:\n",
    "    print(f\"\\n=== QUERY: {query} ===\")\n",
    "\n",
    "    # Search\n",
    "    top_chunks = search_engine.search(query, top_k=TOP_K)\n",
    "\n",
    "    # Print top chunks\n",
    "    print(\"\\nTop Retrieved Chunks:\")\n",
    "    for i, chunk in enumerate(top_chunks):\n",
    "        print(f\"\\n--- Chunk {i+1} ---\")\n",
    "        print(f\"{chunk['chunk']}\")\n",
    "        print(f\"Metadata: {chunk['metadata']}\")\n",
    "\n",
    "    # Generate Answer\n",
    "    answer = generate_answer(query, top_chunks)\n",
    "    print(\"\\nGenerated Answer:\")\n",
    "    print(answer)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "177a8ceb-7230-4f26-824d-356c47f378a5",
   "metadata": {},
   "source": [
    "### Query wise execution & generate results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6d05085-8941-4a32-9bb2-3a416568637a",
   "metadata": {},
   "source": [
    "### Self designed Queries\n",
    "\n",
    "Here are the 3 required queries (as used in main.py):\n",
    "\n",
    "1. What summary does the thread provide about delays in project delivery?\n",
    "\n",
    "2. What decision was made about budget increase in email thread about resource allocation?\n",
    "\n",
    "3. Was the procurement request for new laptops approved?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5856201c-4ee8-4f8e-8015-984268e170ae",
   "metadata": {},
   "source": [
    "### Query1 : What summary does the thread provide about delays in project delivery?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "bcfea6b1-4b1d-4dd9-aaaf-2bb2ffb5d746",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== QUERY: What summary does the thread provide about delays in project delivery? ===\n",
      "\n",
      "Top Retrieved Chunks:\n",
      "\n",
      "--- Chunk 1 ---\n",
      "Hi Bob, We are experiencing delays in Project Falcon due to supplier issues. Expect 2-week delay. Regards, Alice\n",
      "Metadata: {'from': 'alice@example.com', 'subject': 'Project Falcon Delay', 'thread_id': 1001, 'timestamp': '2025-07-01 09:15:00'}\n",
      "\n",
      "--- Chunk 2 ---\n",
      "Requesting remote work extension till July 31 due to personal reasons. -Tina\n",
      "Metadata: {'from': 'tina@example.com', 'subject': 'Remote Work Extension Request', 'thread_id': 1009, 'timestamp': '2025-07-12 09:30:00'}\n",
      "\n",
      "--- Chunk 3 ---\n",
      "Thanks for the update. Can you send revised timeline? Thanks, Bob\n",
      "Metadata: {'from': 'bob@example.com', 'subject': 'Project Falcon Delay', 'thread_id': 1001, 'timestamp': '2025-07-01 10:00:00'}\n"
     ]
    }
   ],
   "source": [
    "query1 = \"What summary does the thread provide about delays in project delivery?\"\n",
    "TOP_K = 3\n",
    "\n",
    "print(f\"\\n=== QUERY: {query1} ===\")\n",
    "\n",
    "# Search Layer outputs\n",
    "top_chunks1 = search_engine.search(query1, top_k=TOP_K)\n",
    "\n",
    "# Print top chunks\n",
    "print(\"\\nTop Retrieved Chunks:\")\n",
    "for i, chunk in enumerate(top_chunks1):\n",
    "    print(f\"\\n--- Chunk {i+1} ---\")\n",
    "    print(f\"{chunk['chunk']}\")\n",
    "    print(f\"Metadata: {chunk['metadata']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "96106d14-2a8e-411f-b3b8-cd7cf0253db0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Generated Layer Output:\n",
      "The thread indicates delays in Project Falcon due to supplier issues, resulting in a 2-week delay. Tina requested a remote work extension until July 31.\n"
     ]
    }
   ],
   "source": [
    "# Generatove layer Outputs\n",
    "open_ai_output1 = generate_answer(query1, top_chunks1)\n",
    "print(\"\\nGenerated Layer Output:\")\n",
    "print(open_ai_output1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b23165dd-de9b-4518-879b-a5d82ca9ad78",
   "metadata": {},
   "source": [
    "### Query2 : What decision was made about budget increase in email thread about resource allocation?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "c9eb889e-4592-4b14-a464-497c11cedf18",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== QUERY: What decision was made about budget increase in email thread about resource allocation? ===\n",
      "\n",
      "Top Retrieved Chunks:\n",
      "\n",
      "--- Chunk 1 ---\n",
      "Finance approved 10% increase in marketing for Q3. Please adjust campaigns accordingly. Carol\n",
      "Metadata: {'from': 'carol@example.com', 'subject': 'Q3 Marketing Budget', 'thread_id': 1002, 'timestamp': '2025-06-15 14:30:00'}\n",
      "\n",
      "--- Chunk 2 ---\n",
      "Thanks for the update. Can you send revised timeline? Thanks, Bob\n",
      "Metadata: {'from': 'bob@example.com', 'subject': 'Project Falcon Delay', 'thread_id': 1001, 'timestamp': '2025-07-01 10:00:00'}\n",
      "\n",
      "--- Chunk 3 ---\n",
      "Some receipts were missing. Please upload them to get full reimbursement. -Accounts\n",
      "Metadata: {'from': 'accounts@example.com', 'subject': 'Expense Report Clarification', 'thread_id': 1010, 'timestamp': '2025-08-01 14:30:00'}\n"
     ]
    }
   ],
   "source": [
    "query2 = \"What decision was made about budget increase in email thread about resource allocation?\"\n",
    "TOP_K = 3\n",
    "\n",
    "print(f\"\\n=== QUERY: {query2} ===\")\n",
    "\n",
    "# Search Layer outputs\n",
    "top_chunks2 = search_engine.search(query2, top_k=TOP_K)\n",
    "\n",
    "# Print top chunks\n",
    "print(\"\\nTop Retrieved Chunks:\")\n",
    "for i, chunk in enumerate(top_chunks2):\n",
    "    print(f\"\\n--- Chunk {i+1} ---\")\n",
    "    print(f\"{chunk['chunk']}\")\n",
    "    print(f\"Metadata: {chunk['metadata']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "2f803925-f4d8-4b33-88ae-0374c78f8af1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Generated Layer Output:\n",
      "A 10% increase in marketing budget for Q3 was approved by Finance.\n"
     ]
    }
   ],
   "source": [
    "# Generatove layer Outputs\n",
    "open_ai_output2 = generate_answer(query2, top_chunks2)\n",
    "print(\"\\nGenerated Layer Output:\")\n",
    "print(open_ai_output2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2d7f937-605c-4d7b-99aa-bdac3e444d3f",
   "metadata": {},
   "source": [
    "### Query3 : Was the procurement request for new laptops approved?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "d7199c50-4e01-405b-9ec1-03dfaa922be9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== QUERY: Was the procurement request for new laptops approved? ===\n",
      "\n",
      "Top Retrieved Chunks:\n",
      "\n",
      "--- Chunk 1 ---\n",
      "Requesting approval for purchasing new laptops for dev team. Cost: ₹3,00,000. -Admin\n",
      "Metadata: {'from': 'admin@example.com', 'subject': 'Procurement Request Approval', 'thread_id': 1008, 'timestamp': '2025-07-28 10:00:00'}\n",
      "\n",
      "--- Chunk 2 ---\n",
      "Requesting remote work extension till July 31 due to personal reasons. -Tina\n",
      "Metadata: {'from': 'tina@example.com', 'subject': 'Remote Work Extension Request', 'thread_id': 1009, 'timestamp': '2025-07-12 09:30:00'}\n",
      "\n",
      "--- Chunk 3 ---\n",
      "Kavita Sharma will join as intern in product team starting 5th Aug. Recruiter\n",
      "Metadata: {'from': 'recruiter@example.com', 'subject': 'New Intern Joining', 'thread_id': 1004, 'timestamp': '2025-08-01 12:00:00'}\n"
     ]
    }
   ],
   "source": [
    "query3 = \"Was the procurement request for new laptops approved?\"\n",
    "TOP_K = 3\n",
    "\n",
    "print(f\"\\n=== QUERY: {query3} ===\")\n",
    "\n",
    "# Search Layer outputs\n",
    "top_chunks3 = search_engine.search(query3, top_k=TOP_K)\n",
    "\n",
    "# Print top chunks\n",
    "print(\"\\nTop Retrieved Chunks:\")\n",
    "for i, chunk in enumerate(top_chunks3):\n",
    "    print(f\"\\n--- Chunk {i+1} ---\")\n",
    "    print(f\"{chunk['chunk']}\")\n",
    "    print(f\"Metadata: {chunk['metadata']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "de0060a9-2923-4250-9f68-a304df06ecc3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Generated Layer Output:\n",
      "The procurement request for new laptops for the dev team at a cost of ₹3,00,000 was not explicitly addressed in the email thread excerpts provided.\n"
     ]
    }
   ],
   "source": [
    "# Generatove layer Outputs\n",
    "open_ai_output3 = generate_answer(query3, top_chunks3)\n",
    "print(\"\\nGenerated Layer Output:\")\n",
    "print(open_ai_output3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ead0875-2b93-416e-9a32-2bfdc38c3c14",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0f87e55-ce8d-413f-b6d9-36423c5d9ec5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c6b0a6f1-6997-4151-90f8-2051d602ba77",
   "metadata": {},
   "source": [
    "## Batch Evaluator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bdaeb81-e8eb-4533-894d-d2e1425a7c49",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from evaluate import load\n",
    "#from src.search_layer import SearchEngine\n",
    "#from src.generation_layer import generate_answer\n",
    "\n",
    "# Load ROUGE metric\n",
    "rouge = load(\"rouge\")\n",
    "\n",
    "# Load datasets\n",
    "threads_df = pd.read_csv(\"email_dataset/email_threads.csv\")\n",
    "summaries_df = pd.read_csv(\"email_dataset/email_summaries.csv\")\n",
    "\n",
    "# Initialize search engine\n",
    "#search_engine = SearchEngine()\n",
    "\n",
    "# Number of samples to test (limit for speed, e.g., 10)\n",
    "N = 10\n",
    "\n",
    "# Collect results\n",
    "results = []\n",
    "\n",
    "for i in tqdm(range(N)):\n",
    "    row = summaries_df.iloc[i]\n",
    "    thread_id = row['thread_id']\n",
    "    reference_summary = row['summary']\n",
    "\n",
    "    # Get full email body for the thread\n",
    "    thread_emails = threads_df[threads_df['thread_id'] == thread_id]\n",
    "    email_texts = thread_emails['body'].tolist()\n",
    "\n",
    "    if not email_texts:\n",
    "        continue\n",
    "\n",
    "    query = \"Summarize the key decisions made in this email thread.\"\n",
    "    # Perform search on chunks\n",
    "    top_chunks = search_engine.search(query, top_k=3, filter_thread_id=thread_id)\n",
    "\n",
    "    # If no chunks found, skip\n",
    "    if not top_chunks:\n",
    "        continue\n",
    "\n",
    "    # Generate answer from chunks\n",
    "    try:\n",
    "        generated_summary = generate_answer(query, top_chunks)\n",
    "    except Exception as e:\n",
    "        print(f\"Error generating summary: {e}\")\n",
    "        continue\n",
    "\n",
    "    # Compute ROUGE scores\n",
    "    rouge_scores = rouge.compute(predictions=[generated_summary], references=[reference_summary])\n",
    "\n",
    "    results.append({\n",
    "        \"thread_id\": thread_id,\n",
    "        \"query\": query,\n",
    "        \"reference_summary\": reference_summary,\n",
    "        \"generated_summary\": generated_summary,\n",
    "        \"rouge1\": rouge_scores[\"rouge1\"],\n",
    "        \"rouge2\": rouge_scores[\"rouge2\"],\n",
    "        \"rougeL\": rouge_scores[\"rougeL\"]\n",
    "    })\n",
    "\n",
    "# Save to CSV\n",
    "results_df = pd.DataFrame(results)\n",
    "results_df.to_csv(\"batch_evaluation_results.csv\", index=False)\n",
    "\n",
    "print(\"✅ Batch evaluation complete. Results saved to `batch_evaluation_results.csv`.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9edc0ba1-94ed-4acf-b5c6-1f56e88c02f1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "9db84971-56fc-4680-ac11-730566ae5280",
   "metadata": {},
   "source": [
    "## Future Enhancements"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "991ce0ef-99a4-4378-af07-733ebcf39843",
   "metadata": {},
   "source": [
    "**Embedding Layer Enhancements:**\n",
    "\n",
    "- Parallelize or batch chunking and embedding for large datasets.\n",
    "\n",
    "- Support multilingual email embedding using a multilingual transformer model (e.g., distiluse-base-multilingual-cased).\n",
    "\n",
    "- Add logging and error handling during embedding and chunking.\n",
    "\n",
    "- Deduplicate similar chunks before storing in the vector DB to reduce redundancy.\n",
    "\n",
    "- Store additional metadata (e.g., department, priority) to enable advanced filtering during search."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4409b692-3774-44c5-a11b-18ee19e319e7",
   "metadata": {},
   "source": [
    "**Cahce Layer Enhancements:**\n",
    "\n",
    "- Replace JSON with Redis or SQLite for faster lookup and persistence in multi-user environments.\n",
    "\n",
    "- Add cache eviction policy (e.g., LRU) to avoid unlimited growth.\n",
    "\n",
    "- Track cache hit/miss stats for performance analytics.\n",
    "\n",
    "- Encrypt cache contents if storing sensitive queries or responses."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba40cebb-df47-43dd-bad0-7ebfe03d5a07",
   "metadata": {},
   "source": [
    "**Search Layer Enhancements:**\n",
    "\n",
    "- Improve reranking with better cross-encoders like bge-reranker or cohere models.\n",
    "\n",
    "- Add semantic filters beyond thread_id (e.g., date, sender, topic).\n",
    "\n",
    "- Support multi-query or follow-up query handling (e.g., thread-based QA).\n",
    "\n",
    "- Paginate results and allow sorting based on relevance, timestamp, etc.\n",
    "\n",
    "- Expose the search as an API with configurable parameters."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43260ebe-cd16-448f-a618-92d406e88455",
   "metadata": {},
   "source": [
    "**Generation Layer Enhancements:**\n",
    "\n",
    "- Use function calling / structured output instead of plain text (for automation).\n",
    "\n",
    "- Support custom prompt templates per use case (summarization, classification, etc.).\n",
    "\n",
    "- Switch to a self-hosted model (e.g., LLaMA 3, Mistral) for cost and privacy control.\n",
    "\n",
    "- Limit token count dynamically to avoid truncation of large prompts.\n",
    "\n",
    "- Stream responses if using GPT-4-turbo for better UX."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e188ca78-36f6-4946-9a7f-70aff1635b28",
   "metadata": {},
   "source": [
    "**Overall Architecture Enhancements**\n",
    "\n",
    "- Centralized logging and monitoring (e.g., using logging, Sentry, or Prometheus).\n",
    "\n",
    "- Unit and integration tests for all layers to ensure robustness.\n",
    "\n",
    "- Add retry mechanisms for external API calls (OpenAI, Chroma).\n",
    "\n",
    "- Implement role-based access control (RBAC) if deployed in an enterprise environment.\n",
    "\n",
    "- Deploy as a containerized microservice (Docker + FastAPI) with endpoints for embedding, search, and generation.\n",
    "\n",
    "- Add a front-end interface for uploading emails, searching threads, and viewing generated insights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6f5d6bd-f845-4bf9-8b91-21799567ff0e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
